{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "210711_Tensorflow_딥러닝_자연어처리_2강",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOwdDJxaqtUJMILNerO0u5Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changyong93/TIL/blob/main/210711_Tensorflow_%EB%94%A5%EB%9F%AC%EB%8B%9D_%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC_2%EA%B0%95.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMKd0Ol6y3pz"
      },
      "source": [
        "# 선형회귀 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HduzDQdgxT8w"
      },
      "source": [
        "## 자동 미분을 이용한 선형 회귀 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSWtWZYOxYOV"
      },
      "source": [
        "### 자동 미분"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52Xcg72Qxb54"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZhuYXg2zUKC"
      },
      "source": [
        "tape_gradient()는 자동 미분 기능을 수행. 임의로 2w^2+5라는 식을 세워보고 w에 대해 미분"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWfox1BFxdd3"
      },
      "source": [
        "w = tf.Variable(2.)\n",
        "# tensorflow type으로 변수 선언\n",
        "# tensor(텐서)를 메모리에 저장하는 변수\n",
        "\n",
        "def f(w):\n",
        "    y = w**2\n",
        "    z = 2*y + 5\n",
        "    return z"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDXXInySxjtB"
      },
      "source": [
        "이제 gradients를 출력하면 w에 대한 미분한 값이 저장된 것을 확인할 수 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfDlgtk4yOsW",
        "outputId": "cf4b4f7a-4eae-4563-8710-d8c6ee9847ab"
      },
      "source": [
        "# tf.GradientTape는 컨텍스트(context) 안에서 실행된 모든 연산을 테이프(tape)에 \"기록\"\n",
        "# https://www.tensorflow.org/guide/autodiff?hl=ko\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    z = f(w)\n",
        "\n",
        "gradients = tape.gradient(z, [w])\n",
        "# z를 w에 대해 미분을 하는데, 이때 w가 2일 때의 미분 값을 출력\n",
        "print(gradients)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor: shape=(), dtype=float32, numpy=8.0>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fY7k86gn0DWa"
      },
      "source": [
        "### 선형 회귀 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RO5kanC92CSJ",
        "outputId": "24dca0f4-6e2d-4a72-ab8b-7232e61a1377"
      },
      "source": [
        "# 선형 회귀 모델(Wx+b)를 위한 tf.Variable을 선언합니다\n",
        "W = tf.Variable(tf.random.normal(shape = [1])) #랜덤으로 초기값 지정\n",
        "b = tf.Variable(tf.random.normal(shape = [1])) #랜덤으로 초기값 지정\n",
        "print(W)\n",
        "print(b)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([-1.091866], dtype=float32)>\n",
            "<tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.16852678], dtype=float32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hiWfjsu3f6C"
      },
      "source": [
        "#### 가설 정의\n",
        "---\n",
        "@tf.function을 추가하면 파이썬 코드가 동일한 텐서플로우 코드로 변경되며, 이렇게 하면 성능을 최적화 할 수 있음  \n",
        "모든 파이썬 함수에 tf.funciton 데코레이터를 적용 할 필요는 없음.   \n",
        "모델 훈련의 한 단계(step)나 정방향 연산(forward pass) 같은 고수준 연산에만 tf.function 데코레이터를 적용\n",
        "\n",
        "---\n",
        "데코레이션\n",
        "- http://solarisailab.com/archives/2351 : 텐서플로우 기초 그래프 생성과 실행 설명\n",
        "- https://www.tensorflow.org/guide/function?hl=ko : (정식 문서)\n",
        "- https://www.inflearn.com/questions/174459 : tf.functions 역할 설명(인프런)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w1j9kEABPYK"
      },
      "source": [
        "#여기서 정방향연산은 입력을 넣었을 때 결과를 출력하는 아래 linear_model 함수\n",
        "@tf.function\n",
        "def linear_model(x):\n",
        "    return W*x + b"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5z-pQD7mBT0P"
      },
      "source": [
        "#### 손실 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mmzj_dMBW7a"
      },
      "source": [
        "#손실 함수를 정의\n",
        "#MSE 손실함수 사용(mean(y - yy)^2)\n",
        "@tf.function\n",
        "def mse_loss(y_pred,y): #제곱한 값이므로, y와 y' 위치가 바뀌어도 상관없음\n",
        "    return tf.reduce_mean(tf.square(y_pred - y)) #tf.reduce_mean = 평균을 출력해주는 메소드"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyL69Zf_B0MJ"
      },
      "source": [
        "#### 옵티마이저 정의(경사 하강법)\n",
        "\n",
        "파라미터를 업데이트 하는 한 순간을 step이라고 할 때, 그 한 순간을  정의한 함수를 train_step이라는 이름의 함수로 작성   \n",
        "아래 내용은 이해가 안되면 일단 처음에는 암기를 해도 무방할 만큼, TF2.0에서는 당연하게 사용되는 패턴   \n",
        "앞으로의 강의에서는 이보다도 더 쉬운 keras 패턴을 사용할 예정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOqBUAPNCL14"
      },
      "source": [
        "# 최적화를 위한 그라디언트 디센트 옵티마이저 정의\n",
        "# 사용자가 정한 learning rate 값을 sgd()에 작성\n",
        "optimizer = tf.optimizers.SGD(learning_rate=0.01)\n",
        "\n",
        "# 첫 번째 인자인 x는 입력 데이터\n",
        "# 두 번째 인자인 y는 레이블\n",
        "@tf.function\n",
        "def train_step(x,y):\n",
        "  with tf.GradientTape() as tape:\n",
        "    #현재 파라미터에 기반한 입력 x에 대한 예측값을 y_pred\n",
        "    y_pred = linear_model(x)\n",
        "    \n",
        "    #MSE 계산\n",
        "    loss = mse_loss(y_pred,y)\n",
        "\n",
        "  #손실 함수에 대한 파라미터의 미분값 계산\n",
        "  gradients = tape.gradient(loss,[W,b]) #loss를 W와 b에 대해서 미분\n",
        "\n",
        "  #파라미터 업데이트\n",
        "  optimizer.apply_gradients(zip(gradients, [W,b])) #미분한 값을 기준으로 W,b를 업데이트"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xQSnc80EUF1",
        "outputId": "f6097129-ff50-4a74-d7c4-5a282126a7ce"
      },
      "source": [
        "# 트레이닝을 위한 입력값과 출력값 준비\n",
        "x_train = [1,2,3,4]\n",
        "y_train = [2,4,6,8]\n",
        "\n",
        "# 경사하강법을 1000번 수행\n",
        "for i in range(1000):\n",
        "  if (i+1) % 100 == 0:\n",
        "    print(i+1, \"회 학습\")\n",
        "  train_step(x_train,y_train)\n",
        "\n",
        "# 테스트를 위한 입력값을 준비\n",
        "x_test = [3.5, 5, 5.5, 6]\n",
        "# 테스트 데이터를 이요해 학습된 선형회귀 모델이 데이터의 경향성(y = 2x)을 잘 학습했는지 측정\n",
        "\n",
        "# 예상되는 참값 : [7, 10, 11, 12]\n",
        "# print(linear_model(x_test))\n",
        "print(linear_model(x_test).numpy())"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 회 학습\n",
            "200 회 학습\n",
            "300 회 학습\n",
            "400 회 학습\n",
            "500 회 학습\n",
            "600 회 학습\n",
            "700 회 학습\n",
            "800 회 학습\n",
            "900 회 학습\n",
            "1000 회 학습\n",
            "[ 6.9896154  9.961792  10.9525175 11.943244 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ay1UfkjxWPo"
      },
      "source": [
        "## 케라스를 이용한 선형 회귀 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPtR5-nk014O"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import optimizers"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awSJej2xIgjE"
      },
      "source": [
        "keras 모델을 만드는 방법은 sequential 방법과 functional API 방법이 있다(https://leestation.tistory.com/777, https://subinium.github.io/Keras-7/)\n",
        "- Sequential : 층층이(layer-by-layer) 쌓아 올릴 수 있게 하는 방법\n",
        "  - 대부분의 문제를 해결할 수 있지만, layer를 공유하는 구조나, 다중 입력/출력을 사용하지 못하는 문제가 있다\n",
        "- functional API : layer가 앞/뒤 layer에만 연결된 구조뿐 아니라 훨씬 더 자유자재로 그 구조를 정의하여 사용 가능\n",
        "  - layer들을 어떤 layer에든지 연결하여 사용 가능\n",
        "---\n",
        "- 첫 번째 인자인 1은 출력 차원을 정의\n",
        "- 두 번째 인자인 input_dim은 입력 차원을 정의\n",
        "- 이번 실습과 같이 1개의 실수 x를 가지고 하는 1개의 실수 y를 예측하는 단순 선형 회귀를 구현하는 경우에는 각각 1의 값을 가짐"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dohd5Mw0GzYv",
        "outputId": "8ad44db9-5c2a-45c6-f1a4-4926ea3a1d3f"
      },
      "source": [
        "X=[1,2,3,4,5,6,7,8,9] # 공부하는 시간\n",
        "y=[11,22,33,44,53,66,77,87,95] # 각 공부하는 시간에 맵핑되는 성적\n",
        "\n",
        "#방식 1-1)\n",
        "model = Sequential()\n",
        "\n",
        "# 입력 x의 차원은 1, 출력 y의 차원도 1. 선형 회귀이므로 activation은 'linear'\n",
        "#1-1)\n",
        "model.add(Dense(units = 1, input_dim = 1, activation = 'linear'))\n",
        "#1-2)\n",
        "# model.add(Dense(units = 1, input_shape = (1,), activation = 'linear'))\n",
        "\n",
        "#방식 2)\n",
        "# model = Sequential(\n",
        "#     [\n",
        "#      Dense(units = 1, input_dim = 1,activation = 'linear')\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "# sgd는 경하 하강법을 의미. 학습률(learning rate,lr)은 0.01\n",
        "sgd = optimizers.SGD(lr=0.01)\n",
        "\n",
        "\n",
        "# 손실 함수(Loss function)은 평균제곱오차 mse를 사용\n",
        "model.compile(optimizer = sgd, loss = 'mse', metrics = ['mse'])\n",
        "\n",
        "model.fit(X,y, batch_size = 1, epochs = 300, shuffle = False)\n",
        "#shuffle은 batch_size 단위로 섞음\n",
        "#shuffle을 안할 시 입력 데이터 순서까지 학습할 수 있음\n",
        "#일반적으로 데이터 순서가 무의미 하기에 셔플을 적용하지만 수업에선 셔플 옵션이 있다는 것을 보여주기 위해 False를 함"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 308.0792 - mse: 308.0792\n",
            "Epoch 2/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1537 - mse: 2.1537\n",
            "Epoch 3/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1534 - mse: 2.1534\n",
            "Epoch 4/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1532 - mse: 2.1532\n",
            "Epoch 5/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1530 - mse: 2.1530\n",
            "Epoch 6/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1527 - mse: 2.1527\n",
            "Epoch 7/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1525 - mse: 2.1525\n",
            "Epoch 8/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1523 - mse: 2.1523\n",
            "Epoch 9/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1521 - mse: 2.1521\n",
            "Epoch 10/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1519 - mse: 2.1519\n",
            "Epoch 11/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1517 - mse: 2.1517\n",
            "Epoch 12/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1515 - mse: 2.1515\n",
            "Epoch 13/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1513 - mse: 2.1513\n",
            "Epoch 14/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1511 - mse: 2.1511\n",
            "Epoch 15/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1510 - mse: 2.1510\n",
            "Epoch 16/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1508 - mse: 2.1508\n",
            "Epoch 17/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1506 - mse: 2.1506\n",
            "Epoch 18/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1505 - mse: 2.1505\n",
            "Epoch 19/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1504 - mse: 2.1504\n",
            "Epoch 20/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1502 - mse: 2.1502\n",
            "Epoch 21/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1501 - mse: 2.1501\n",
            "Epoch 22/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1499 - mse: 2.1499\n",
            "Epoch 23/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1498 - mse: 2.1498\n",
            "Epoch 24/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1497 - mse: 2.1497\n",
            "Epoch 25/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1496 - mse: 2.1496\n",
            "Epoch 26/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1494 - mse: 2.1494\n",
            "Epoch 27/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1493 - mse: 2.1493\n",
            "Epoch 28/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1492 - mse: 2.1492\n",
            "Epoch 29/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1491 - mse: 2.1491\n",
            "Epoch 30/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1490 - mse: 2.1490\n",
            "Epoch 31/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1489 - mse: 2.1489\n",
            "Epoch 32/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1488 - mse: 2.1488\n",
            "Epoch 33/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1487 - mse: 2.1487\n",
            "Epoch 34/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1486 - mse: 2.1486\n",
            "Epoch 35/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1486 - mse: 2.1486\n",
            "Epoch 36/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1485 - mse: 2.1485\n",
            "Epoch 37/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1484 - mse: 2.1484\n",
            "Epoch 38/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1483 - mse: 2.1483\n",
            "Epoch 39/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1482 - mse: 2.1482\n",
            "Epoch 40/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1482 - mse: 2.1482\n",
            "Epoch 41/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1481 - mse: 2.1481\n",
            "Epoch 42/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1480 - mse: 2.1480\n",
            "Epoch 43/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1480 - mse: 2.1480\n",
            "Epoch 44/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1479 - mse: 2.1479\n",
            "Epoch 45/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1478 - mse: 2.1478\n",
            "Epoch 46/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1478 - mse: 2.1478\n",
            "Epoch 47/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1477 - mse: 2.1477\n",
            "Epoch 48/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1476 - mse: 2.1476\n",
            "Epoch 49/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1476 - mse: 2.1476\n",
            "Epoch 50/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1476 - mse: 2.1476\n",
            "Epoch 51/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1475 - mse: 2.1475\n",
            "Epoch 52/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1475 - mse: 2.1475\n",
            "Epoch 53/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1474 - mse: 2.1474\n",
            "Epoch 54/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1474 - mse: 2.1474\n",
            "Epoch 55/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1473 - mse: 2.1473\n",
            "Epoch 56/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1473 - mse: 2.1473\n",
            "Epoch 57/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1472 - mse: 2.1472\n",
            "Epoch 58/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1472 - mse: 2.1472\n",
            "Epoch 59/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1472 - mse: 2.1472\n",
            "Epoch 60/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1471 - mse: 2.1471\n",
            "Epoch 61/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1471 - mse: 2.1471\n",
            "Epoch 62/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1470 - mse: 2.1470\n",
            "Epoch 63/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1470 - mse: 2.1470\n",
            "Epoch 64/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1470 - mse: 2.1470\n",
            "Epoch 65/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1469 - mse: 2.1469\n",
            "Epoch 66/300\n",
            "9/9 [==============================] - 0s 3ms/step - loss: 2.1469 - mse: 2.1469\n",
            "Epoch 67/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1469 - mse: 2.1469\n",
            "Epoch 68/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1469 - mse: 2.1469\n",
            "Epoch 69/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1468 - mse: 2.1468\n",
            "Epoch 70/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1468 - mse: 2.1468\n",
            "Epoch 71/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1468 - mse: 2.1468\n",
            "Epoch 72/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1468 - mse: 2.1468\n",
            "Epoch 73/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1467 - mse: 2.1467\n",
            "Epoch 74/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1467 - mse: 2.1467\n",
            "Epoch 75/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1467 - mse: 2.1467\n",
            "Epoch 76/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1467 - mse: 2.1467\n",
            "Epoch 77/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1466 - mse: 2.1466\n",
            "Epoch 78/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1466 - mse: 2.1466\n",
            "Epoch 79/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1466 - mse: 2.1466\n",
            "Epoch 80/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1466 - mse: 2.1466\n",
            "Epoch 81/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1466 - mse: 2.1466\n",
            "Epoch 82/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1465 - mse: 2.1465\n",
            "Epoch 83/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1465 - mse: 2.1465\n",
            "Epoch 84/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1465 - mse: 2.1465\n",
            "Epoch 85/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1465 - mse: 2.1465\n",
            "Epoch 86/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1465 - mse: 2.1465\n",
            "Epoch 87/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1465 - mse: 2.1465\n",
            "Epoch 88/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1464 - mse: 2.1464\n",
            "Epoch 89/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1464 - mse: 2.1464\n",
            "Epoch 90/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1464 - mse: 2.1464\n",
            "Epoch 91/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1464 - mse: 2.1464\n",
            "Epoch 92/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1464 - mse: 2.1464\n",
            "Epoch 93/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1464 - mse: 2.1464\n",
            "Epoch 94/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1464 - mse: 2.1464\n",
            "Epoch 95/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1464 - mse: 2.1464\n",
            "Epoch 96/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1463 - mse: 2.1463\n",
            "Epoch 97/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1463 - mse: 2.1463\n",
            "Epoch 98/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1463 - mse: 2.1463\n",
            "Epoch 99/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1463 - mse: 2.1463\n",
            "Epoch 100/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1463 - mse: 2.1463\n",
            "Epoch 101/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1463 - mse: 2.1463\n",
            "Epoch 102/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1463 - mse: 2.1463\n",
            "Epoch 103/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1463 - mse: 2.1463\n",
            "Epoch 104/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1463 - mse: 2.1463\n",
            "Epoch 105/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1463 - mse: 2.1463\n",
            "Epoch 106/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1463 - mse: 2.1463\n",
            "Epoch 107/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1462 - mse: 2.1462\n",
            "Epoch 108/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1462 - mse: 2.1462\n",
            "Epoch 109/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1462 - mse: 2.1462\n",
            "Epoch 110/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1462 - mse: 2.1462\n",
            "Epoch 111/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1462 - mse: 2.1462\n",
            "Epoch 112/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1462 - mse: 2.1462\n",
            "Epoch 113/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1462 - mse: 2.1462\n",
            "Epoch 114/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1462 - mse: 2.1462\n",
            "Epoch 115/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1462 - mse: 2.1462\n",
            "Epoch 116/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1462 - mse: 2.1462\n",
            "Epoch 117/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1462 - mse: 2.1462\n",
            "Epoch 118/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1462 - mse: 2.1462\n",
            "Epoch 119/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1462 - mse: 2.1462\n",
            "Epoch 120/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1462 - mse: 2.1462\n",
            "Epoch 121/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1462 - mse: 2.1462\n",
            "Epoch 122/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1462 - mse: 2.1462\n",
            "Epoch 123/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1462 - mse: 2.1462\n",
            "Epoch 124/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1462 - mse: 2.1462\n",
            "Epoch 125/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 126/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 127/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 128/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 129/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 130/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 131/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 132/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 133/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 134/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 135/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 136/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 137/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 138/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 139/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 140/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 141/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 142/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 143/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 144/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 145/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 146/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 147/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 148/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 149/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 150/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 151/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 152/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 153/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 154/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 155/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 156/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 157/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 158/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 159/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 160/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 161/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 162/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 163/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 164/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 165/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 166/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 167/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 168/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 169/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 170/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 171/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1461 - mse: 2.1461\n",
            "Epoch 172/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 173/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 174/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 175/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 176/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 177/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 178/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 179/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 180/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 181/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 182/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 183/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 184/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 185/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 186/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 187/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 188/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 189/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 190/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 191/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 192/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 193/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 194/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 195/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 196/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 197/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 198/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 199/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 200/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 201/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 202/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 203/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 204/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 205/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 206/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 207/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 208/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 209/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 210/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 211/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 212/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 213/300\n",
            "9/9 [==============================] - 0s 3ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 214/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 215/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 216/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 217/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 218/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 219/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 220/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 221/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 222/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 223/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 224/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 225/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 226/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 227/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 228/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 229/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 230/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 231/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 232/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 233/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 234/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 235/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 236/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 237/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 238/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 239/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 240/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 241/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 242/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 243/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 244/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 245/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 246/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 247/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 248/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 249/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 250/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 251/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 252/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 253/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 254/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 255/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 256/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 257/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 258/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 259/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 260/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 261/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 262/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 263/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 264/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 265/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 266/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 267/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 268/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 269/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 270/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 271/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 272/300\n",
            "9/9 [==============================] - 0s 3ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 273/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 274/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 275/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 276/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 277/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 278/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 279/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 280/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 281/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 282/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 283/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 284/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 285/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 286/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 287/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 288/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 289/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 290/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 291/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 292/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 293/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 294/300\n",
            "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 295/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 296/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 297/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 298/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 299/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n",
            "Epoch 300/300\n",
            "9/9 [==============================] - 0s 2ms/step - loss: 2.1460 - mse: 2.1460\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7987d81f50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "nt2R0uj0Tt0s",
        "outputId": "87458c66-d591-45c3-f445-1aace701cbf7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(X,model.predict(X),'b',X,y,'k--')"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f7985513e90>,\n",
              " <matplotlib.lines.Line2D at 0x7f7985521090>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}