{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "딥러닝을_이용한_자연어처리_입문(12_5_NLP를 위한 신경망(CNN)_Multi-Kernel 1D CNN으로 네이버 영화 리뷰 분류하기",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN+WNPbUM/chBl8sBEQfLI2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changyong93/Natural-language-processing-with-chat-bot/blob/main/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%84_%EC%9D%B4%EC%9A%A9%ED%95%9C_%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC_%EC%9E%85%EB%AC%B8(12_5_NLP%EB%A5%BC_%EC%9C%84%ED%95%9C_%EC%8B%A0%EA%B2%BD%EB%A7%9D(CNN)_Multi_Kernel_1D_CNN%EC%9C%BC%EB%A1%9C_%EB%84%A4%EC%9D%B4%EB%B2%84_%EC%98%81%ED%99%94_%EB%A6%AC%EB%B7%B0_%EB%B6%84%EB%A5%98%ED%95%98%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yLB78V2Uuvd"
      },
      "source": [
        "# Multi-Kernel 1D CNN으로 네이버 영화 리뷰 분류하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaaem1BYVNKV"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4i5OW69RVBwd"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import urllib.request\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LzL-MVdVcmw"
      },
      "source": [
        "## 데이터 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOwUPdD2VhD8"
      },
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtY1WMcSVoZ_"
      },
      "source": [
        "train_data = pd.read_table('ratings_train.txt')\n",
        "test_data = pd.read_table('ratings_test.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgiHhib_Vu5L"
      },
      "source": [
        "print(\"훈련용 데이터 개수: \", train_data.shape[0])\n",
        "print(\"테스트용 데이터 개수: \", test_data.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCHCUCKCV5w-"
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mza6S2AVV8os"
      },
      "source": [
        "test_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9BjRN-UWABF"
      },
      "source": [
        "## 데이터 정제"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIexr-JEWCo5"
      },
      "source": [
        "train_data.nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bWF2E7gWIOP"
      },
      "source": [
        "train_data = train_data.drop_duplicates(subset = [\"document\"])\n",
        "train_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-rQTkh0WSrP"
      },
      "source": [
        "train_data.label.value_counts().reset_index(name = \"count\")\n",
        "train_data.label.value_counts().plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R-Z88d8WqBW"
      },
      "source": [
        "train_data.isnull().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCAXMwtLWtmX"
      },
      "source": [
        "train_data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpBzCallWv-U"
      },
      "source": [
        "train_data[train_data.document.isnull()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdD63x83W0sN"
      },
      "source": [
        "train_data = train_data.dropna(how = 'any')\n",
        "train_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E42ipgNqW8z2"
      },
      "source": [
        "train_data[\"document\"] = train_data.document.str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPvsxrxFXuRV"
      },
      "source": [
        "train_data[\"document\"] = train_data[\"document\"].str.replace(\"^ +\", \"\")\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boVmaJ1OX7bm"
      },
      "source": [
        "train_data[\"document\"] = train_data[\"document\"].replace(\"\",np.nan)\n",
        "train_data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYbFeOVSYmSe"
      },
      "source": [
        "train_data[train_data[\"document\"].isnull()].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwGCd4RhY422"
      },
      "source": [
        "train_data = train_data.dropna(how = 'any')\n",
        "train_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM_g7o7CZA-L"
      },
      "source": [
        "print(test_data.shape)\n",
        "test_data = test_data.drop_duplicates(subset = ['document'])\n",
        "test_data[\"document\"] = test_data[\"document\"].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\")\n",
        "test_data[\"document\"] = test_data[\"document\"].str.replace(\"^ +\", \"\")\n",
        "test_data[\"document\"] = test_data[\"document\"].replace(\"\", np.nan)\n",
        "test_data = test_data.dropna(how = 'any')\n",
        "print(test_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrD6trjFZdbc"
      },
      "source": [
        "## 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuoxpKrzaq4i"
      },
      "source": [
        "okt = Okt()\n",
        "train_data[\"tokenized\"] = train_data[\"document\"].apply(lambda x : okt.morphs(x,stem = True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7TXgRTycDHT"
      },
      "source": [
        "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
        "train_data[\"tokenized\"] = train_data[\"tokenized\"].map(lambda x : [word for word in x if not word in stopwords])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBd8WTIGdqC2"
      },
      "source": [
        "train_data[\"tokenized\"][:5].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkE6pjR7dw6j"
      },
      "source": [
        "test_data[\"tokenized\"] = test_data[\"document\"].apply(lambda x : okt.morphs(x,stem = True))\n",
        "test_data[\"tokenized\"] = test_data[\"tokenized\"].map(lambda x : [word for word in x if not word in stopwords])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAFVRWurd9QA"
      },
      "source": [
        "test_data[\"tokenized\"][:5].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykvKy6vgd_9V"
      },
      "source": [
        "## 정수인코딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J94XjNh1eCYl"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_data[\"tokenized\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-awViWEeMsV"
      },
      "source": [
        "print(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiDYOIaReQAP"
      },
      "source": [
        "threshold = 3\n",
        "total_cnt = len(tokenizer.word_index)\n",
        "total_freq = 0\n",
        "\n",
        "rare_cnt = 0\n",
        "rare_freq = 0\n",
        "for key,value in tokenizer.word_counts.items():\n",
        "  total_freq += value\n",
        "  if value < 3:\n",
        "    rare_cnt += 1\n",
        "    rare_freq += value\n",
        "\n",
        "print(\"단어 집합 크기: \",total_cnt)\n",
        "print(f\"등반 빈도가 {threshold} 미만인 희귀 단어 수: {rare_cnt}\")\n",
        "print(f\"전체 단어 중 희귀 단어 비율: {rare_cnt / total_cnt * 100:.3f}\")\n",
        "print(f\"전체 등장 빈도 중 희귀 단어 비율: {rare_freq / total_freq * 100:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-wmjmCe3JID"
      },
      "source": [
        "vocab_size = total_cnt - rare_cnt + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQCAvikd3SYj"
      },
      "source": [
        "vocab_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qIViu9K3S_l"
      },
      "source": [
        "tokenizer = Tokenizer(num_words = vocab_size)\n",
        "tokenizer.fit_on_texts(train_data[\"tokenized\"])\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(train_data[\"tokenized\"])\n",
        "X_test = tokenizer.texts_to_sequences(test_data[\"tokenized\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6fDRyiX3nbH"
      },
      "source": [
        "y_train = np.array(train_data[\"label\"])\n",
        "y_test = np.array(test_data[\"label\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDQ1u77043FT"
      },
      "source": [
        "## 빈 샘플 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Dwk6JOq4398"
      },
      "source": [
        "drop_train = [idx for idx, sentence in enumerate(X_train) if len(sentence) < 1]\n",
        "\n",
        "#빈 샘플 제거\n",
        "X_train = np.delete(X_train, drop_train, axis = 0)\n",
        "y_train = np.delete(y_train, drop_train, axis = 0)\n",
        "\n",
        "print(len(X_train), len(y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMmb8Sqr5-5b"
      },
      "source": [
        "## 패딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWCwbvQG6SFX"
      },
      "source": [
        "max_len = max([len(s) for s in X_train])\n",
        "len_list = [len(s) for s in X_train]\n",
        "for length in range(1,max_len+1):\n",
        "  cnt = 0\n",
        "  for s in X_train:\n",
        "    if len(s) <= length:\n",
        "      cnt +=1\n",
        "  print(f\"샘플 중 길이가 {length} 이하인 샘플의 비율: {cnt / len(len_list)*100:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoLoqcRO7tPn"
      },
      "source": [
        "max_len = 30\n",
        "X_train = pad_sequences(sequences=X_train, maxlen = max_len)\n",
        "X_test = pad_sequences(sequences=X_test, maxlen = max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjoZ5WFG71wU"
      },
      "source": [
        "## Multi-Kernel 1D CNN으로 네이버 영화 리뷰 분류하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGjHnVN1h-yL"
      },
      "source": [
        "print(X_train.shape)\n",
        "X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSVbG72eofaB"
      },
      "source": [
        "print(y_train.shape)\n",
        "y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7G6ukgp7_rY"
      },
      "source": [
        "from tensorflow.keras.models import load_model, Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalMaxPooling1D, Dropout, Conv1D, Input, Flatten, Concatenate\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B1rbnR185cq"
      },
      "source": [
        "# 하이퍼 파라미터 정의\n",
        "embedding_dim = 128\n",
        "dropout_prob = (0.5, 0.8)\n",
        "num_filters = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MabOjNDn8-lX"
      },
      "source": [
        "# 입력층과 임베딩층 정의\n",
        "# 임베딩층 이후 50% 드랍아웃\n",
        "\n",
        "model_input = Input(shape = (max_len,))\n",
        "print(model_input)\n",
        "z = Embedding(input_dim = vocab_size, output_dim = embedding_dim,input_length = max_len, name = \"embedding\")(model_input)\n",
        "print(z)\n",
        "z = Dropout(dropout_prob[0])(z)\n",
        "print(z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWnZ58kbeu24"
      },
      "source": [
        "conv_blocks = []\n",
        "\n",
        "for sz in [3,4,5]:\n",
        "  conv = Conv1D(filters = num_filters,\n",
        "                kernel_size = sz,\n",
        "                padding = \"valid\",\n",
        "                activation = \"relu\",\n",
        "                strides = 1)(z)\n",
        "  print(\"Con1D\",sz,conv)\n",
        "  conv = GlobalMaxPooling1D()(conv)\n",
        "  print(conv)\n",
        "  conv = Flatten()(conv)\n",
        "  print(conv)\n",
        "  conv_blocks.append(conv)\n",
        "  print(\"*\"*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SC0K8AtEqo-W"
      },
      "source": [
        "z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
        "print(z)\n",
        "z = Dropout(dropout_prob[1])(z)\n",
        "print(z)\n",
        "z = Dense(units = 128, activation = \"relu\")(z)\n",
        "print(z)\n",
        "model_output = Dense(units = 1, activation = 'sigmoid')(z)\n",
        "print(model_output)\n",
        "\n",
        "model = Model(model_input, model_output) #훈련 및 예측을 위해 여러 개 layer를 하나의 객체에 저장\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFJCZobdz306"
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('CNN__model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['acc'])\n",
        "model.fit(x = X_train, y = y_train, batch_size = 64, epochs = 10, validation_data=(X_test,y_test),verbose = 2,callbacks = [es,mc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEDbR2tbz5Lt"
      },
      "source": [
        "loaded_model = load_model(filepath = \"CNN_model.h5\")\n",
        "\n",
        "print(f\"\\n테스트 정확도: {loaded_model.evaluate(x = X_test,y = y_test)[1]:.3f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSwOsW50sPMs"
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('CNN_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['acc'])\n",
        "model.fit(x = X_train, y = y_train, batch_size = 64, epochs = 10, validation_data=(X_test,y_test),verbose = 2,callbacks = [es,mc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnW4b1SvzPr3"
      },
      "source": [
        "loaded_model = load_model(filepath = \"CNN_model.h5\")\n",
        "\n",
        "print(f\"\\n테스트 정확도: {loaded_model.evaluate(x = X_test,y = y_test)[1]:.3f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp4WtT4C2FDl"
      },
      "source": [
        "## 리뷰 예측"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBja2s842Hm0"
      },
      "source": [
        "def sentiment_predict(new_sentence):\n",
        "  new_sentence = okt.morphs(new_sentence,stem=True) #토큰화\n",
        "  new_sentence = [word for word in new_sentence if not word in stopwords] #불용어 처리\n",
        "  encoded = tokenizer.texts_to_sequences([new_sentence]) #정수화\n",
        "  pad_new = pad_sequences(encoded,maxlen = max_len)\n",
        "  score = float(loaded_model.predict(pad_new))\n",
        "  if score > 0.5 :\n",
        "    print(f\"{score*100:.3f}% 확률로 긍정 리뷰입니다.\")\n",
        "  else:\n",
        "    print(f\"{(1-score)*100:.3f}% 확률로 부정 리뷰입니다.\")\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyZy5Nag3i3P"
      },
      "source": [
        "sentiment_predict('이 영화 개꿀잼 ㅋㅋㅋ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e5Utxin3kCR"
      },
      "source": [
        "sentiment_predict('이 영화 핵노잼 ㅠㅠ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDfrclEm3186"
      },
      "source": [
        "sentiment_predict('이딴게 영화냐 ㅉㅉ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2_InGUD328t"
      },
      "source": [
        "sentiment_predict('감독 뭐하는 놈이냐?')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QDm28r334gp"
      },
      "source": [
        "sentiment_predict('와 개쩐다 정말 세계관 최강자들의 영화다')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMIwJleFhsR8"
      },
      "source": [
        "# import tensorflow as tf\n",
        "# x = np.arange(20).reshape(2, 2, 5)\n",
        "# print(x)\n",
        "# print(\"*\"*100)\n",
        "# y = np.arange(20, 40).reshape(2, 2, 5)\n",
        "# print(y)\n",
        "# print(\"*\"*100)\n",
        "# print(tf.keras.layers.Concatenate(axis=-1)([x, y]))\n",
        "# print(\"*\"*100)\n",
        "# print(tf.keras.layers.Concatenate(axis=1)([x, y]))\n",
        "# print(\"*\"*100)\n",
        "# print(tf.keras.layers.Concatenate(axis=0)([x, y]))\n",
        "# print(\"*\"*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egDQvVgQpsWJ"
      },
      "source": [
        "print(tf.keras.layers.Concatenate(axis=-1)([x, y]))\n",
        "Flatten()(tf.keras.layers.Concatenate(axis=-1)([x, y]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}