{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "딥러닝을_이용한_자연어처리_입문(13_1~2_NLP를 위한 신경망(CNN)_양방향 LSTM을 이용한 품사 태깅",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM3HzBnp50PXQ1Q85QsP0q9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changyong93/Natural-language-processing-with-chat-bot/blob/main/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%84_%EC%9D%B4%EC%9A%A9%ED%95%9C_%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC_%EC%9E%85%EB%AC%B8(13_1~2_NLP%EB%A5%BC_%EC%9C%84%ED%95%9C_%EC%8B%A0%EA%B2%BD%EB%A7%9D(CNN)_%EC%96%91%EB%B0%A9%ED%96%A5_LSTM%EC%9D%84_%EC%9D%B4%EC%9A%A9%ED%95%9C_%ED%92%88%EC%82%AC_%ED%83%9C%EA%B9%85.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmrbm5eOlSAC"
      },
      "source": [
        "# 양방향 LSTM을 이용한 품사 태깅(Part-of-speech Tagging using Bi-LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6iMH65S0-SY"
      },
      "source": [
        "## 품사 태깅 데이터에 대한 이해와 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bl_J9m8a1CxV"
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OwP1Zzi1uGd"
      },
      "source": [
        "nltk.download('treebank')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBOZSHdu1jSG"
      },
      "source": [
        "tagged_sentences = nltk.corpus.treebank.tagged_sents() # 토큰화에 품사 태깅이 된 데이터 받아오기\n",
        "print(\"품사 태깅이 된 문장 개수: \", len(tagged_sentences)) # 문장 샘플의 개수 출력"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6IWECqN2iU7"
      },
      "source": [
        "print(tagged_sentences[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kpnflr1S2wKq"
      },
      "source": [
        "sentences, pos_tags = [], [] \n",
        "for tagged_sentence in tagged_sentences:\n",
        "  sentence, tag_info = zip(*tagged_sentence)\n",
        "  sentences.append(list(sentence))\n",
        "  pos_tags.append(list(tag_info))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuAd_b8236cS"
      },
      "source": [
        "print(sentences[0])\n",
        "print(pos_tags[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea1uFCt239Kg"
      },
      "source": [
        "print('샘플의 최대 길이 : %d' % max(len(l) for l in sentences))\n",
        "print('샘플의 평균 길이 : %f' % (sum(map(len, sentences))/len(sentences)))\n",
        "plt.hist([len(s) for s in sentences], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DDpdbFj4CQf"
      },
      "source": [
        "def tokenize(samples):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(samples)\n",
        "  return tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYTqfZu44Mdo"
      },
      "source": [
        "src_tokenizer = tokenize(sentences)\n",
        "tar_tokenizer = tokenize(pos_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ey6lUAD44Nd6"
      },
      "source": [
        "vocab_size = len(src_tokenizer.word_index) + 1\n",
        "tag_size = len(tar_tokenizer.word_index) + 1\n",
        "print('단어 집합의 크기 : {}'.format(vocab_size))\n",
        "print('태깅 정보 집합의 크기 : {}'.format(tag_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5rldpBu4Pdz"
      },
      "source": [
        "X_train = src_tokenizer.texts_to_sequences(sentences)\n",
        "y_train = tar_tokenizer.texts_to_sequences(pos_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HquKGJ9K4ScR"
      },
      "source": [
        "max_len = 150\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=max_len)\n",
        "y_train = pad_sequences(y_train, padding='post', maxlen=max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADapIp0s4fUS"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=.2, random_state=777)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQS1-bLf4fic"
      },
      "source": [
        "y_train = to_categorical(y_train, num_classes=tag_size)\n",
        "y_test = to_categorical(y_test, num_classes=tag_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WlOHB7i4q_H"
      },
      "source": [
        "print('훈련 샘플 문장의 크기 : {}'.format(X_train.shape))\n",
        "print('훈련 샘플 레이블의 크기 : {}'.format(y_train.shape))\n",
        "print('테스트 샘플 문장의 크기 : {}'.format(X_test.shape))\n",
        "print('테스트 샘플 레이블의 크기 : {}'.format(y_test.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSSw9awQ4rNv"
      },
      "source": [
        "## 양방향 LSTM(Bi-directional LSTM)으로 POS Tagger 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RTX3R_O4tdX"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Bidirectional, Embedding, LSTM,TimeDistributed\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLM_6ubN8u9l"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim = vocab_size, output_dim = 128, input_length = max_len, mask_zero = True))\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "# model.add(TimeDistributed(Dense(tag_size, activation=('softmax')))) # many to many에서는 각 시간대의 해당 셀에 대한 hidden_state를 아웃풋으로 전달할 때 TimeDistreibuted를 써야 했지만 이제는 불필요odel = Sequential()\n",
        "model.add(Dense(tag_size, activation=('softmax')))\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eal6wBjp-Mqo"
      },
      "source": [
        "model.fit(X_train, y_train, batch_size=128, epochs=6,  validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ob270rV8-QXv"
      },
      "source": [
        "print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJZPwDEHCEAQ"
      },
      "source": [
        "index_to_word=src_tokenizer.index_word\n",
        "index_to_tag=tar_tokenizer.index_word\n",
        "\n",
        "i= 10\n",
        "y_pred = model.predict(np.array([X_test[i]]))\n",
        "y_pred = np.argmax(y_pred,axis = -1)\n",
        "true = np.argmax(y_test[i], axis = -1)  \n",
        "\n",
        "print(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\n",
        "print(35 * \"-\")\n",
        "\n",
        "for w, t, pred in zip(X_test[i], true, y_pred[0]):\n",
        "    if w != 0: # PAD값은 제외함.\n",
        "        print(\"{:17}: {:7} {}\".format(index_to_word[w], index_to_tag[t].upper(), index_to_tag[pred].upper()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLP1jqdVDA3h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}