{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "딥러닝을_이용한_자연어처리_입문(11_7_RNN을_이용한_텍스트분류_네이버 쇼핑 리뷰 감성 분류",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP/2DL7VrBVGRavvmog6V90",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changyong93/Natural-language-processing-with-chat-bot/blob/main/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%84_%EC%9D%B4%EC%9A%A9%ED%95%9C_%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC_%EC%9E%85%EB%AC%B8(11_7_RNN%EC%9D%84_%EC%9D%B4%EC%9A%A9%ED%95%9C_%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%B6%84%EB%A5%98_%EB%84%A4%EC%9D%B4%EB%B2%84_%EC%87%BC%ED%95%91_%EB%A6%AC%EB%B7%B0_%EA%B0%90%EC%84%B1_%EB%B6%84%EB%A5%98.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgOsZLqYkEJQ"
      },
      "source": [
        "# 네이버 쇼핑 리뷰 감성 분류하기(Naver Shopping Review Sentiment Analysis)\n",
        "---\n",
        "- 다운로드 링크 : https://github.com/bab2min/corpus/tree/master/sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tE20L0sWkOxc"
      },
      "source": [
        "#konlpy 설치\n",
        "!pip install konlpy\n",
        "\n",
        "#mecab 설치\n",
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "%cd Mecab-ko-for-Google-Colab\n",
        "!bash install_mecab-ko_on_colab190912.sh\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "from collections import Counter\n",
        "from konlpy.tag import Mecab\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr2u6OOMlRSv"
      },
      "source": [
        "## 네이버 쇼핑 리뷰 데이터에 대한 이해와 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndY-QOsmlZkP"
      },
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/bab2min/corpus/master/sentiment/naver_shopping.txt\", filename=\"ratings_total.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_IGkQjal69p"
      },
      "source": [
        "total_data = pd.read_table(\"ratings_total.txt\", names = ['rating','reviews'])\n",
        "total_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYJduI8jmJRE"
      },
      "source": [
        "print(\"전체 데이터 개수: \", len(total_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtWHAxsZnoPQ"
      },
      "source": [
        "### train & test 데이터 나누기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMny1UGqmM8R"
      },
      "source": [
        "total_data['label'] = (total_data.rating >= 3).astype(int)\n",
        "# total_data['label'] = np.select([total_data.rating > 3], [1],0)\n",
        "\n",
        "total_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y52nWFOWnkCS"
      },
      "source": [
        "print(total_data.shape)\n",
        "print(total_data.nunique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fdr-hvurn5wt"
      },
      "source": [
        "#중복 제거\n",
        "print(\"총 샘플 수(중복 제거 전)\", len(total_data))\n",
        "total_data = total_data.drop_duplicates(subset = [\"reviews\"]).copy()\n",
        "print(\"총 샘플 수(중복 제거 후)\", len(total_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIXBOBgXoNVM"
      },
      "source": [
        "#null\n",
        "total_data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuLZJ4HYoRvq"
      },
      "source": [
        "train_data, test_data = train_test_split(total_data, test_size = 0.25, random_state = 42)\n",
        "train_data.shape, test_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3ExsNmwoqOb"
      },
      "source": [
        "### 분포 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9Jpyqggoucf"
      },
      "source": [
        "train_data.label.value_counts().plot.bar()\n",
        "train_data.label.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaWsBEr-o7ax"
      },
      "source": [
        "### 데이터 정제"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNkN4oSjphQl"
      },
      "source": [
        "train_data[\"reviews\"] = train_data[\"reviews\"].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
        "train_data[\"reviews\"] = train_data[\"reviews\"].replace(\"^ +\",\"\")\n",
        "train_data[\"reviews\"] = train_data[\"reviews\"].replace(\"\", np.nan)\n",
        "train_data = train_data.dropna(how = 'any').copy()\n",
        "\n",
        "test_data = test_data.drop_duplicates(subset = [\"reviews\"]).copy()\n",
        "test_data[\"reviews\"] = test_data[\"reviews\"].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\")\n",
        "test_data[\"reviews\"] = test_data[\"reviews\"].replace(\"^ +\", \"\")\n",
        "test_data[\"reviews\"] = test_data[\"reviews\"].replace(\"\", np.nan)\n",
        "test_data = test_data.dropna(how = 'any').copy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CnKfaTFq8Yf"
      },
      "source": [
        "print(\"테스트 데이터 개수: \", len(test_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R84b-flTpoSt"
      },
      "source": [
        "### 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Lat-MH6r3QA"
      },
      "source": [
        "mecab = Mecab()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8U88SSd2rz3"
      },
      "source": [
        "print(mecab.morphs('와 이런 것도 상품이라고 차라리 내가 만드는 게 나을 뻔'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIYcgysb27rG"
      },
      "source": [
        "#stopwords 지정\n",
        "stopwords = ['도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를', '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk9FqH6b3Bfj"
      },
      "source": [
        "train_data[\"tokenized\"] = train_data[\"reviews\"].apply(mecab.morphs)\n",
        "train_data[\"tokenized\"] = train_data[\"tokenized\"].apply(lambda x : [word for word in x if word not in stopwords])\n",
        "\n",
        "test_data[\"tokenized\"] = test_data[\"reviews\"].apply(mecab.morphs)\n",
        "test_data[\"tokenized\"] = test_data[\"tokenized\"].apply(lambda x : [word for word in x if word not in stopwords])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSOhF3s43j1x"
      },
      "source": [
        "### 단어 길이와 분포 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSwBZf2T3m_2"
      },
      "source": [
        "negative = np.hstack(train_data.loc[train_data.label==0,\"tokenized\"].values)\n",
        "positive = np.hstack(train_data.loc[train_data.label==1,\"tokenized\"].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6NYdFfq4cqf"
      },
      "source": [
        "negative_word_count = Counter(negative)\n",
        "print(negative_word_count.most_common(20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzRyiaoc4pEa"
      },
      "source": [
        "positive_word_count = Counter(positive)\n",
        "print(positive_word_count.most_common(20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7PMg7XN4wQY"
      },
      "source": [
        "bin_num = 20\n",
        "fig,(ax1,ax2) = plt.subplots(1,2,figsize = (12,5))\n",
        "# [len(text) for text in train_data.loc[train_data.label ==0, \"tokenized\"]]\n",
        "text_len = train_data.loc[train_data.label ==0, \"tokenized\"].apply(len)\n",
        "ax1.hist(text_len, bins = bin_num, color = 'red')\n",
        "ax1.set_xlabel(\"length of samples\")\n",
        "ax1.set_ylabel(\"number of samples\")\n",
        "ax1.set_title(\"Negative Reviews\")\n",
        "print(\"부정 리뷰 평균 길이: \",round(np.mean(text_len),3))\n",
        "print(\"부정 리뷰 최대 길이: \",max(text_len))\n",
        "\n",
        "text_len = train_data.loc[train_data.label ==1, \"tokenized\"].apply(len)\n",
        "ax2.hist(text_len, bins = bin_num, color = 'blue')\n",
        "ax2.set_xlabel(\"length of samples\")\n",
        "ax2.set_ylabel(\"number of samples\")\n",
        "ax2.set_title(\"Positive Reviews\")\n",
        "print(\"긍정 리뷰 평균 길이: \",round(np.mean(text_len),3))\n",
        "print(\"긍정 리뷰 최대 길이: \",max(text_len))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcbwa-_E6sb6"
      },
      "source": [
        "X_train = train_data.tokenized.values\n",
        "y_train = train_data.label.values\n",
        "\n",
        "X_test = test_data.tokenized.values\n",
        "y_test = test_data.label.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf5ZVDFy7R8b"
      },
      "source": [
        "### 정수인코딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhJp7JRL9d34"
      },
      "source": [
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8zf_0Jw7UmC"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4EKq-MH-bKj"
      },
      "source": [
        "len(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bapL0lqx7doP"
      },
      "source": [
        "#단어 빈도수가 1개인 것 제거\n",
        "threshold = 2\n",
        "total_cnt = len(tokenizer.word_index)\n",
        "rare_cnt = 0 # 전체 단어 중 빈도수가 2 미만인 개수 카운트\n",
        "total_freq = 0 #전체 단어의 빈도수\n",
        "rare_freq = 0 #rare_cnt에 해당하는 단어의 빈도수\n",
        "\n",
        "for key,value in tokenizer.word_counts.items():\n",
        "  total_freq += value\n",
        "\n",
        "  if value < threshold:\n",
        "    rare_cnt += 1\n",
        "    rare_freq += value\n",
        "\n",
        "print(\"전체 단어 집합(vocabulary) 크기: \", total_cnt)\n",
        "print(\"전체 단어 중 등장 빈도가 1개인 단어의 개수: \", rare_cnt)\n",
        "print(f\"전체 단어에서 희귀 단어의 비율: {rare_cnt / total_cnt * 100  :.3f}%\")\n",
        "print(f\"전체 단어 빈도에서 희귀 단어 빈도 비율: {rare_freq / total_freq * 100 :.3f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwKoI63H-Yaa"
      },
      "source": [
        "#등장 빈도수 1 이하인 단어 제외\n",
        "# 0번 패딩 토큰과 1번 OOV 토큰을 고려하여 +2\n",
        "vocab_size = total_cnt - rare_cnt + 2\n",
        "print(\"단어 집합의 크기: \",vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzckbenD_Ox2"
      },
      "source": [
        "#정수인코딩\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token = \"OOV\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Z7_4kCQ_dCk"
      },
      "source": [
        "print(X_train[:3])\n",
        "print(\"*\"*100)\n",
        "print(X_test[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4noXrJFv_ihw"
      },
      "source": [
        "### 패딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKVmmZ-4_vsu"
      },
      "source": [
        "print(\"리뷰 최대 길이: \", max([len(s) for s in X_train]))\n",
        "print(\"리뷰 평균 길이: \", np.mean([len(s) for s in X_train]))\n",
        "\n",
        "plt.hist([len(s) for s in X_train], bins = 50)\n",
        "plt.xlabel(\"length of samples\")\n",
        "plt.ylabel(\"number of samples\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djncmW_gAWhX"
      },
      "source": [
        "max_len = max([len(s) for s in X_train])\n",
        "\n",
        "for lens in range(1,max_len+1):\n",
        "  cnt = sum([1 for idx in range(len(X_train)) if len(X_train[idx]) <= lens])\n",
        "  print(f\"전체 샘플 중 전체 길이가 {lens}이하인 샘플의 비율: {cnt / len(X_train) * 100 :.3f}%\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK1gb2GABRez"
      },
      "source": [
        "max_len = 80\n",
        "X_train = pad_sequences(sequences=X_train, maxlen = max_len)\n",
        "X_test = pad_sequences(sequences=X_test, maxlen = max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sxms2RnBxpl"
      },
      "source": [
        "## GRU로 네이버 쇼핑 리뷰 감성 분류"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7jy4MqFC1zu"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding, Dense, GRU\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8A48ONNDH3I"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim = vocab_size, output_dim = 100))\n",
        "model.add(GRU(units = 128))\n",
        "model.add(Dense(units = 1, activation = 'sigmoid'))\n",
        "\n",
        "es = EarlyStopping(monitor = 'val_loss', patience = 4, verbose = 1, mode = \"min\")\n",
        "mc = ModelCheckpoint(filepath = \"base_model.h5\", monitor='val_acc',mode = \"max\",save_best_only=True, verbose = 1)\n",
        "\n",
        "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy',metrics = ['acc'])\n",
        "history = model.fit(x = X_train, y = y_train, batch_size = 60, epochs = 15, validation_split=0.2, callbacks = [es, mc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4c4MmgaELvG"
      },
      "source": [
        "loaded_model = load_model(\"base_model.h5\")\n",
        "print(f\"테스트 정확도: {loaded_model.evaluate(X_test,y_test)[1]:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlHK8LzTNGZ1"
      },
      "source": [
        "## 리뷰 예측"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I_Pub9vNIst"
      },
      "source": [
        "def sentiment_predict(new_sentence):\n",
        "  new_sentence = mecab.morphs(new_sentence) #토큰화\n",
        "  new_sentence = [word for word in new_sentence if word not in stopwords] #불용어 제거\n",
        "  encoded = tokenizer.texts_to_sequences([new_sentence]) #정수 인코딩\n",
        "  pad_new = pad_sequences(encoded, maxlen = max_len)\n",
        "  score = float(loaded_model.predict(pad_new))\n",
        "  if score > 0.5:\n",
        "    print(f\"{score*100:.2f}% 확률로 긍정 리뷰입니다.\")\n",
        "  else:\n",
        "    print(f\"{(1-score)*100:.2f}확률로 확률로 긍정 리뷰입니다.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDVZNx_bQeJJ"
      },
      "source": [
        "sentiment_predict('이 상품 진짜 좋아요... 저는 강추합니다. 대박')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtAD1VuwR8QC"
      },
      "source": [
        "sentiment_predict('진짜 배송도 늦고 개짜증나네요. 뭐 이런 걸 상품이라고 만듬?')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSqZIXlUR8X_"
      },
      "source": [
        "sentiment_predict('판매자님... 너무 짱이에요.. 대박나삼')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCLuoI-5RgLQ"
      },
      "source": [
        "sentiment_predict('ㅁㄴㅇㄻㄴㅇㄻㄴㅇ리뷰쓰기도 귀찮아')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQfg7x8AR_A4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}