{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "딥러닝을_이용한_자연어처리_입문(11_6_RNN을_이용한_텍스트분류_네이버 영화 리뷰 감성 분류)",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMhC2efABmCSlBl4PQFGh3z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changyong93/Natural-language-processing-with-chat-bot/blob/main/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%84_%EC%9D%B4%EC%9A%A9%ED%95%9C_%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC_%EC%9E%85%EB%AC%B8(11_6_RNN%EC%9D%84_%EC%9D%B4%EC%9A%A9%ED%95%9C_%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%B6%84%EB%A5%98_%EB%84%A4%EC%9D%B4%EB%B2%84_%EC%98%81%ED%99%94_%EB%A6%AC%EB%B7%B0_%EA%B0%90%EC%84%B1_%EB%B6%84%EB%A5%98).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pL_fJuEj9Vs_"
      },
      "source": [
        "# 네이버 영화 리뷰 감성 분류하기(Naver Movie Review Sentiment Analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-jwPfeB9iUo"
      },
      "source": [
        "## 네이버 영화 리뷰 데이터에 대한 이해와 전처리\n",
        "- 다운로드 링크 : https://github.com/e9t/nsmc/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWE6Wlu09lTC"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import urllib.request\n",
        "\n",
        "!pip install konlpy\n",
        "from konlpy.tag import Okt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCR5Tz-O_PJR"
      },
      "source": [
        "## 데이터 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jl08n__L-JAc"
      },
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SQwG2oz-m78"
      },
      "source": [
        "train_data = pd.read_table(\"ratings_train.txt\")\n",
        "test_data = pd.read_table(\"ratings_test.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5f-_vGG-x7y"
      },
      "source": [
        "#훈련용 개수 출력\n",
        "print(\"훈련용 데이터 개수: \", len(train_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sU4A241T-4Cz"
      },
      "source": [
        "#상위 5개 항목 출력\n",
        "train_data.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdbmFvYy-7cY"
      },
      "source": [
        "#테스트 데이터 개수\n",
        "print(\"테스트 데이터 개수: \", test_data.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzSiP4Yi_HMF"
      },
      "source": [
        "#테스트 데이터 상위 5개\n",
        "test_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcFY4YDo_Jf1"
      },
      "source": [
        "## 데이터 정제"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvjVUlNA_SCI"
      },
      "source": [
        "print(train_data.shape)\n",
        "print(\"*\"*100)\n",
        "print(train_data.nunique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQsP8E-o_ebo"
      },
      "source": [
        "train_data = train_data.drop_duplicates(subset = [\"document\"]).copy()\n",
        "train_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSMW6R0U_mWV"
      },
      "source": [
        "train_data.label.value_counts().plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MKFAp0X_uKq"
      },
      "source": [
        "train_data.groupby(\"label\").size().reset_index(name = \"count\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Os35aC50Ez7C"
      },
      "source": [
        "### 결측치 처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lB-WFp1H_-30"
      },
      "source": [
        "#결측치 확인\n",
        "train_data.isnull().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zScH-AA7AO03"
      },
      "source": [
        "train_data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CF55oIRDAUQw"
      },
      "source": [
        "train_data[train_data.document.isnull()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOorXCmXAZFz"
      },
      "source": [
        "#결측치 제거\n",
        "train_data = train_data.dropna(how = \"any\").copy()\n",
        "print(train_data.shape)\n",
        "print(train_data.isnull().any())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYKBkLk1ExLF"
      },
      "source": [
        "### 데이터 전처리 시작"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "me__u1cvAp8e"
      },
      "source": [
        "#알파벳과 공백을 제외하고 제거\n",
        "train_data[\"document\"] = train_data[\"document\"].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
        "train_data.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dL6HeT7BNdK"
      },
      "source": [
        "train_data.tail(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS_2-G_kBQx7"
      },
      "source": [
        "# 만약 리뷰가 영어, 숫자, 특수문자로 되어있더라면 현재 공백(white space)만 있거나 빈 값을 가진 행이 생성됨\n",
        "# 상기 내용에 대해 다시 한 번 전처리하여 Null값 처리 후 재확인\n",
        "print(train_data.isnull().sum())\n",
        "train_data[train_data.document.str.contains(\"^ +\")].head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1F0WlWHcBtZR"
      },
      "source": [
        "train_data[\"document\"] = train_data[\"document\"].str.replace(\"^ +\", \"\").copy()\n",
        "train_data[\"document\"] = train_data[\"document\"].replace(\"\", np.nan)\n",
        "print(train_data.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CExe3XFbCyZ6"
      },
      "source": [
        "train_data[train_data.document.isnull()].head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhEc557SDBCt"
      },
      "source": [
        "#Null 샘플은 데이터 분석에 도움이 안되므로 삭제\n",
        "print(\"결측치 제거 전 데이터 개수: \", train_data.shape[0])\n",
        "train_data = train_data.dropna(how = 'any').copy()\n",
        "print(\"결측치 제거 후 데이터 개수: \", train_data.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqlpsnYbDTry"
      },
      "source": [
        "#테스트 데이터도 동일 과정 진행\n",
        "test_data = test_data.drop_duplicates(subset = [\"document\"]).copy()\n",
        "test_data[\"document\"] = test_data[\"document\"].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\").copy()\n",
        "test_data[\"document\"] = test_data[\"document\"].replace(\"^ +\", \"\").copy()\n",
        "test_data[\"document\"] = test_data[\"document\"].replace(\"\", np.nan).copy()\n",
        "test_data = test_data.dropna(how = \"any\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwCKBONlD_a-"
      },
      "source": [
        "print(test_data.shape)\n",
        "print(\"*\"*100)\n",
        "test_data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UQyfuwtEBbG"
      },
      "source": [
        "### 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7Z8OObRFCAR"
      },
      "source": [
        "#stopwords 지정\n",
        "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
        "\n",
        "#형태소 분석기 Okt\n",
        "coupus = '와 이런 것도 영화라고 차라리 뮤직비디오를 만드는 게 나을 뻔'\n",
        "okt = Okt()\n",
        "print(okt.morphs(coupus))\n",
        "print(okt.morphs(coupus,stem = True)) #stem True 시 단어를 보다 정규화 시켜줌"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cuk42D6dOAIc"
      },
      "source": [
        "X_train = []\n",
        "for sentence in train_data.document:\n",
        "  temp_x = okt.morphs(sentence, stem = True)\n",
        "  temp_x = [word for word in temp_x if word not in stopwords]\n",
        "  X_train.append(temp_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KK6K_oAO3Lw"
      },
      "source": [
        "print(X_train[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLUx-BgtPGnX"
      },
      "source": [
        "X_test = []\n",
        "for sentence in test_data.document:\n",
        "  temp_x = okt.morphs(sentence, stem = True)\n",
        "  temp_x = [word for word in temp_x if word not in stopwords]\n",
        "  X_test.append(temp_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a7Psak-PLg6"
      },
      "source": [
        "## 정수 인코딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqdkZ2mySLRi"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQyb5C9bSTOX"
      },
      "source": [
        "print(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdf3FgyUSVpV"
      },
      "source": [
        "threshold = 3\n",
        "total_cnt = len(tokenizer.word_index)\n",
        "rare_cnt = 0 #등장 빈도수가 threshold보다 적은 단어 개수 카운트hreshold = 3\n",
        "total_freq = 0 # 훈련 데이터셋의 전체 단어 빈도수 합\n",
        "rare_freq = 0 #훈련 데이터셋에 threshold보다 등장 빈도수가 적은 단어 빈도수 합\n",
        "\n",
        "for key,value in tokenizer.word_counts.items():\n",
        "  total_freq += value #단어별 빈도수를 모두 합침\n",
        "\n",
        "  #만약 등장 빈도수가 threshold보다 작으면\n",
        "  if value < threshold:\n",
        "    rare_cnt +=1\n",
        "    rare_freq += value\n",
        "\n",
        "print('단어 집합(vocabulary) 크기: ',total_cnt)\n",
        "print(\"등장 빈도수가 %s 이하인 희귀 단어의 수: %s\" %(threshold-1, rare_cnt))\n",
        "print(f\"전체 집합에서 희귀단어 비율: {rare_cnt/total_cnt*100:.2f}%\")\n",
        "print(f\"전체 집합에서 희귀단어 등장 빈도 비율: {rare_freq/total_freq*100:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecT_DHUSU5kz"
      },
      "source": [
        "#등장빈도수 2이하인 단어 제거\n",
        "vocab_size = total_cnt - rare_cnt + 1\n",
        "print(\"단어 집합의 크기: \",vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OKeIXo9VIw0"
      },
      "source": [
        "# 단어 집합의 크기를 조정하여 텍스트 시퀀스를 토큰 시퀀스로 변환\n",
        "#정수인코딩\n",
        "\n",
        "tokenizer = Tokenizer(num_words = vocab_size)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_ACkHsuVl0p"
      },
      "source": [
        "print(X_train[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLopoagpVnyP"
      },
      "source": [
        "y_train = np.array(train_data[\"label\"])\n",
        "y_test = np.array(test_data[\"label\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzwG90F7V-zD"
      },
      "source": [
        "## 빈 샘플 생성\n",
        "---\n",
        "- 빈도수가 낮은 단어를 삭제하여, 샘플별로 빈 값이 생성됨\n",
        "- 빈 단어를 삭제하는 작업 진행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "micKlrj_WQzq"
      },
      "source": [
        "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fhr-XQ2NXr56"
      },
      "source": [
        "#빈 샘플 데이터 확인\n",
        "X_train = np.delete(X_train, drop_train, axis = 0)\n",
        "y_train = np.delete(y_train, drop_train, axis = 0)\n",
        "\n",
        "print(X_train.shape[0],y_train.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTXRmLpbXvGh"
      },
      "source": [
        "## padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTEZ8S9EYee7"
      },
      "source": [
        "print(\"리뷰의 최대 길이: \", max([len(sentence) for sentence in X_train]))\n",
        "print(\"리뷰의 평균 길이: \", np.mean([len(sentence) for sentence in X_train]))\n",
        "\n",
        "plt.hist([len(sentence) for sentence in X_train], bins = 50)\n",
        "plt.xlabel(\"length of samples\")\n",
        "plt.ylabel(\"number of samples\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmTNTL_TYyOK"
      },
      "source": [
        "max_len = max([len(sentence) for sentence in X_train])\n",
        "\n",
        "\n",
        "for lens in range(1,max_len+1):\n",
        "  cnt = 0\n",
        "  for index in range(len(X_train)):\n",
        "    if len(X_train[index]) <= lens:\n",
        "      cnt += 1\n",
        "  print(f\"전체 샘플 중 길이가 {lens} 이하인 샘플 비율: {cnt / len(X_train) *100 :.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkYFLFKRZ0o1"
      },
      "source": [
        "#샘플 길이 30으로 제한\n",
        "max_len = 30\n",
        "X_train = pad_sequences(sequences = X_train,maxlen = max_len)\n",
        "X_test = pad_sequences(sequences = X_test, maxlen = max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf3dJ5DAa2oD"
      },
      "source": [
        "## LSTM으로 네이버 영화 리뷰 감성 분류"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X_zxB-Wa-GN"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b6Tqh7vcO4I"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim = vocab_size, output_dim = 100))\n",
        "model.add(LSTM(units = 128))\n",
        "model.add(Dense(units = 1, activation = 'sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlMtzaGMdAvf"
      },
      "source": [
        "es = EarlyStopping(monitor = 'val_loss',patience = 4, verbose = 1, mode = \"min\")\n",
        "mc = ModelCheckpoint(filepath = \"base_model.h5\",monitor = 'val_acc',verbose = 1, mode = \"max\", save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3Mxh75fdZoj"
      },
      "source": [
        "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['acc'])\n",
        "history = model.fit(x = X_train, y = y_train, batch_size = 60, epochs = 15, validation_split = 0.2, callbacks = [es,mc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWuGe4xwd56w"
      },
      "source": [
        "loaded_model = load_model(\"base_model.h5\")\n",
        "print(f\"\\n 테스트 정확도: {loaded_model.evaluate(X_test,y_test)[1]:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9VjDeMreNC6"
      },
      "source": [
        "## 리뷰 예측하는 함수 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvCiPdFtfcJ2"
      },
      "source": [
        "def sentiment_predict(new_sentence):\n",
        "  new_sentence = okt.morphs(new_sentence, stem = True) #토큰화\n",
        "  new_sentence = [word for word in new_sentence if word not in stopwords]#불용어 제거\n",
        "  encoded = tokenizer.texts_to_sequences([new_sentence])#정수인코딩\n",
        "  padding = pad_sequences(encoded, maxlen = max_len)#패딩\n",
        "  score = float(loaded_model.predict(padding))\n",
        "  if (score > 0.5):\n",
        "    print(f\"{score*100 :.3f}% 확률로 긍정 리뷰입니다\")\n",
        "  else:\n",
        "    print(f\"{(1-score)*100:.3f}% 확률로 부정 리뷰입니다\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "libZjEyqh6ov"
      },
      "source": [
        "sentiment_predict('이 영화 개꿀잼 ㅋㅋㅋ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Xs8oz1fh7xm"
      },
      "source": [
        "sentiment_predict('이 영화 핵노잼 ㅠㅠ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7q7_PhcjUgA"
      },
      "source": [
        "sentiment_predict('이딴게 영화냐 ㅉㅉ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxPYU21pjWwu"
      },
      "source": [
        "sentiment_predict('감독 뭐하는 놈이냐?')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFEHU9ZYjZEE"
      },
      "source": [
        "sentiment_predict('와 개쩐다 정말 세계관 최강자들의 영화다')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgT8_Lh-jaPK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}