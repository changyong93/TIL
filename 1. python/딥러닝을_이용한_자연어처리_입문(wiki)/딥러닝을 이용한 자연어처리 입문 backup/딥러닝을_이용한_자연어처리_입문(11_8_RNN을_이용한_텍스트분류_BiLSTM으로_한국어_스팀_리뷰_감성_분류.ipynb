{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "딥러닝을_이용한_자연어처리_입문(11_8_RNN을_이용한_텍스트분류_BiLSTM으로 한국어 스팀 리뷰 감성 분류",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPEFx0aL2btDzJW86bPQMnu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changyong93/Natural-language-processing-with-chat-bot/blob/main/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%84_%EC%9D%B4%EC%9A%A9%ED%95%9C_%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC_%EC%9E%85%EB%AC%B8(11_8_RNN%EC%9D%84_%EC%9D%B4%EC%9A%A9%ED%95%9C_%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%B6%84%EB%A5%98_BiLSTM%EC%9C%BC%EB%A1%9C_%ED%95%9C%EA%B5%AD%EC%96%B4_%EC%8A%A4%ED%8C%80_%EB%A6%AC%EB%B7%B0_%EA%B0%90%EC%84%B1_%EB%B6%84%EB%A5%98.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjtS0JWJX4nm"
      },
      "source": [
        "# BiLSTM으로 한국어 스팀 리뷰 감성 분류하기\n",
        "- 다운로드 링크 : https://github.com/bab2min/corpus/tree/master/sentiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G5-zUW0YMJE"
      },
      "source": [
        "## 스팀 리뷰 데이터에 대한 이해와 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1oWuU1KYYTh"
      },
      "source": [
        "# Colab에 Mecab 설치\n",
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "%cd Mecab-ko-for-Google-Colab\n",
        "!bash install_mecab-ko_on_colab190912.sh\n",
        "\n",
        "!pip install konlpy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "from collections import Counter\n",
        "from konlpy.tag import Mecab\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import warnings\n",
        "warnings.filterwarnings(action = 'ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBv097dzY8LB"
      },
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/bab2min/corpus/master/sentiment/steam.txt\", filename=\"steam.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AwBUqpnaSDr"
      },
      "source": [
        "total_data = pd.read_table(filepath_or_buffer=\"steam.txt\", names = [\"label\",\"reviews\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zReYfyLfabSy"
      },
      "source": [
        "print(\"전체 리뷰 개수: \", len(total_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HPITFCtcjJ1"
      },
      "source": [
        "total_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unjpuGnVck67"
      },
      "source": [
        "total_data.nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo79t1N-coT8"
      },
      "source": [
        "#중복 데이터 제거\n",
        "total_data = total_data.drop_duplicates(subset = [\"reviews\"]).copy()\n",
        "print(\"총 샘플 개수: \", total_data.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDQ1LuzWcziu"
      },
      "source": [
        "total_data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5F8aEjkc2Ni"
      },
      "source": [
        "### 훈련셋 테스트셋 나누기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlQtMuNRc_g_"
      },
      "source": [
        "train_data, test_data = train_test_split(total_data, test_size = 0.25, random_state = 42)\n",
        "print(\"훈련 리뷰 개수: \", train_data.shape[0])\n",
        "print(\"테스트 리뷰 개수: \", test_data.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Prh5lZnbdN8U"
      },
      "source": [
        "train_data.label.value_counts().plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBKRe9G5dXQv"
      },
      "source": [
        "train_data.label.value_counts().reset_index(name = \"count\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "up2zMqDtdZRL"
      },
      "source": [
        "### 데이터 정제"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRUSDU-6defi"
      },
      "source": [
        "# 한글을 제외하고 모두 제거, 이 과정에서 빈샘플 확인\n",
        "train_data[\"reviews\"] = train_data[\"reviews\"].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
        "train_data[\"reviews\"] = train_data[\"reviews\"].replace(\"^ +\",\"\")\n",
        "train_data[\"reviews\"] = train_data[\"reviews\"].replace(\"\",np.nan)\n",
        "print(test_data.isnull().sum())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lPMUNTGd3aw"
      },
      "source": [
        "#테스트 데이터도 동일 과정 진행\n",
        "test_data[\"reviews\"] = test_data[\"reviews\"].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
        "test_data[\"reviews\"] = test_data[\"reviews\"].replace(\"^ +\",\"\")\n",
        "test_data[\"reviews\"] = test_data[\"reviews\"].replace(\"\",np.nan)\n",
        "test_data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SixpNjaKePow"
      },
      "source": [
        "#불용어 지정\n",
        "stopwords =\\\n",
        "['도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를', '인', '듯',\n",
        " '과', '와', '네', '들', '듯', '지', '임', '게', '만', '게임', '겜', '되', '음', '면']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNjyjeRJjKqB"
      },
      "source": [
        "### 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL7VMyEYjOqC"
      },
      "source": [
        "mecab = Mecab()\n",
        "train_data[\"tokenized\"] = train_data[\"reviews\"].apply(mecab.morphs)\n",
        "train_data[\"tokenized\"] = train_data[\"tokenized\"].apply(lambda x: [word for word in x if word not in stopwords])\n",
        "\n",
        "test_data[\"tokenized\"] = test_data[\"reviews\"].apply(mecab.morphs)\n",
        "test_data[\"tokenized\"] = test_data[\"tokenized\"].apply(lambda x : [word for word in x if word not in stopwords])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOnhE-WRj2dT"
      },
      "source": [
        "### 단어 길이와 분포"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Weer9SzZj8pu"
      },
      "source": [
        "negative_tokens = np.hstack(train_data.loc[train_data.label == 0,\"tokenized\"].values)\n",
        "positive_tokens = np.hstack(train_data.loc[train_data.label == 1,\"tokenized\"].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cQpVB_MkOIJ"
      },
      "source": [
        "negative_tokens_count = Counter(negative_tokens)\n",
        "positive_tokens_count = Counter(positive_tokens)\n",
        "\n",
        "print(positive_tokens_count.most_common(20))\n",
        "print(\"*\"*100)\n",
        "print(negative_tokens_count.most_common(20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGGNOvhQkpDq"
      },
      "source": [
        "#길이 분포\n",
        "fig,(ax1,ax2) = plt.subplots(1,2,figsize = (12,5))\n",
        "text_len = train_data.loc[train_data[\"label\"]==1,\"tokenized\"].apply(len)\n",
        "ax1.hist(x = text_len, color = \"red\")\n",
        "ax1.set_xlabel(\"length of samples\")\n",
        "ax1.set_ylabel(\"number of samples\")\n",
        "ax1.set_title(\"positive Reviews\")\n",
        "print(\"긍정 리뷰의 평균 길이: \", np.mean(text_len))\n",
        "\n",
        "text_len = train_data.loc[train_data[\"label\"]==0,\"tokenized\"].apply(len)\n",
        "ax2.hist(x = text_len, color = \"blue\")\n",
        "ax2.set_xlabel(\"length of samples\")\n",
        "ax2.set_ylabel(\"number of samples\")\n",
        "ax2.set_title(\"negative Reviews\")\n",
        "print(\"부정 리뷰의 평균 길이: \", np.mean(text_len))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vY3zuv--lssL"
      },
      "source": [
        "X_train = train_data[\"tokenized\"].values\n",
        "y_train = train_data[\"label\"].values\n",
        "\n",
        "X_test = test_data[\"tokenized\"].values\n",
        "y_test = test_data[\"label\"].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NmZ58fxl8pS"
      },
      "source": [
        "# 정수인코딩\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6djEgC1imDM6"
      },
      "source": [
        "threshold = 2\n",
        "total_cnt = len(tokenizer.word_index) # 전체 단어의 수\n",
        "rare_cnt = 0 #등장 빈도수가 threhold보다 작은 단어의 수\n",
        "total_freq = 0 #전체 단어의 등장 빈도수\n",
        "rare_freq = 0 # 희귀 등장 단어의 등장 빈도수\n",
        "\n",
        "for key, value in tokenizer.word_counts.items():\n",
        "  total_freq += value\n",
        "\n",
        "  if value < threshold:\n",
        "    rare_cnt += 1\n",
        "    rare_freq += value\n",
        "\n",
        "print(\"전체 등장 집합(vocabulary) 크기: \", total_cnt)\n",
        "print(f\"전체 단어 중 희귀 단어의 개수: {rare_cnt}\")\n",
        "print(f\"전체 단어 중 희귀 단어의 비율 {rare_cnt / total_cnt *100:.3f}%\")\n",
        "print(f\"전체 등장 빈도수 중 희귀 단어 등장 빈도수 비율 {rare_freq / total_freq * 100:.3f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zpfjeBvnw5O"
      },
      "source": [
        "# 등장 빈도가 1회 이상인 단어는 OOV 처리\n",
        "# 0번 패딩 + OOV 토큰을 고려해 +2\n",
        "vocab_size = total_cnt - rare_cnt +2\n",
        "print(\"단어 집합의 크기: \", vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQVBPqrHoLFg"
      },
      "source": [
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token = \"OOV\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKRo2bOkoXnC"
      },
      "source": [
        "print(X_train[:3])\n",
        "print(\"*\"*100)\n",
        "print(X_test[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aSVQl3moyBy"
      },
      "source": [
        "### 패딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65UtKst3rua0"
      },
      "source": [
        "print(\"리뷰의 최대 길이: \", max(map(len, X_train)))\n",
        "print(\"리뷰의 평균 길이: \", np.mean([len(s) for s in X_train]))\n",
        "\n",
        "plt.hist([len(s) for s in X_train], bins = 50)\n",
        "plt.xlabel(\"length of samples\")\n",
        "plt.ylabel(\"number of samples\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT2IJA6ho07s"
      },
      "source": [
        "max_len = max([len(s) for s in X_train])\n",
        "for idx in range(1,max_len+1):\n",
        "  cnt = 0\n",
        "  for value in [len(s) for s in X_train]:\n",
        "    if value <= idx:\n",
        "      cnt +=1\n",
        "  print(f\"f전체 샘플 중 길이가 {idx}이하인 샘플의 비율 {cnt/len(X_train)*100:.3f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bo0fhxrOrAnf"
      },
      "source": [
        "max_len = 60\n",
        "X_train = pad_sequences(X_train, maxlen = max_len)\n",
        "X_test = pad_sequences(X_test, maxlen = max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_StZDv7s-Fj"
      },
      "source": [
        "## BiLSTM으로 스팀  리뷰 감성 분류"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5sePWNztDdf"
      },
      "source": [
        "import re\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import Dense,LSTM, Bidirectional, Embedding\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6n-wVNhtfRp"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim = vocab_size, output_dim = 100))\n",
        "model.add(Bidirectional(layer = LSTM(units = 100, return_sequences=False)))\n",
        "model.add(Dense(units = 1, activation = \"sigmoid\"))\n",
        "\n",
        "es = EarlyStopping(monitor = 'val_loss', patience = 4, verbose = 1, mode = 'min')\n",
        "mc = ModelCheckpoint(filepath = \"base_model.h5\",monitor = 'val_acc', verbose = 1, save_best_only=True, mode = 'max')\n",
        "\n",
        "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['acc'])\n",
        "model.fit(x = X_train, y = y_train, batch_size = 256, epochs = 15, validation_split=0.2, callbacks = [es,mc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZWn01e2vkz7"
      },
      "source": [
        "loaded_model = load_model(\"base_model.h5\")\n",
        "print(f\"테스트 정확도: {loaded_model.evaluate(x = X_test, y = y_test)[1]:.3f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz2TRLyoyjPU"
      },
      "source": [
        "## 리뷰 예측해보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKBHTly0yt6a"
      },
      "source": [
        "def sentiment_predict(new_sentence):\n",
        "  new_sentence = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣]\",\"\",new_sentence)\n",
        "  new_sentence = mecab.morphs(new_sentence)\n",
        "  new_sentence = [word for word in new_sentence if word not in stopwords]\n",
        "  encoded = tokenizer.texts_to_sequences([new_sentence])\n",
        "  pad_new = pad_sequences(sequences = encoded, maxlen = max_len)\n",
        "  score = float(loaded_model.predict(pad_new))\n",
        "  if score > 0.5:\n",
        "    print(f\"{score * 100:.3f}% 확률로 긍정 리뷰입니다.\")\n",
        "  else:\n",
        "    print(f\"{(1-score) * 100:.3f}% 확률로 부정 리뷰입니다.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EQiqEbn1ARt"
      },
      "source": [
        "sentiment_predict('노잼 ..완전 재미 없음 ㅉㅉ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syeOYGEe1CS3"
      },
      "source": [
        "sentiment_predict('조금 어렵지만 재밌음ㅋㅋ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltpU3sPu1CW-"
      },
      "source": [
        "sentiment_predict('케릭터가 예뻐서 좋아요')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVQDHG-B1Doi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}