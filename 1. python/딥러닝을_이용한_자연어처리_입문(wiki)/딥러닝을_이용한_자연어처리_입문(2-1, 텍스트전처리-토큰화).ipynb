{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "딥러닝을 이용한 자연어처리 입문(토큰화)",
      "provenance": [],
      "collapsed_sections": [
        "imposed-killing"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changyong93/Natural-language-processing-with-chat-bot/blob/main/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%84_%EC%9D%B4%EC%9A%A9%ED%95%9C_%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC_%EC%9E%85%EB%AC%B8(2-1%2C%20%ED%85%8D%EC%8A%A4%ED%8A%B8%EC%A0%84%EC%B2%98%EB%A6%AC-%ED%86%A0%ED%81%B0%ED%99%94).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "proper-jordan"
      },
      "source": [
        "# 토큰화\n",
        "  https://wikidocs.net/21698\n"
      ],
      "id": "proper-jordan"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imposed-killing"
      },
      "source": [
        "## 단어 토큰화\n",
        " - 주어진 코퍼스(corpus)에서 토큰(token)이라 불리는 단위로 나누는 작업을 토큰화\n",
        " - 단어 토큰화 : 토큰의 기준을 단어(word)로 하며, 단어 외에도 단어구, 의미를 갖는 문자열로도 간주됨됨\n",
        "   - 보통 토큰화 작업 시 구두점이나 특수문자를 전부 제거하면 토큰의 의미를 잃어버리는 경우도 발생\n",
        "   - 심지어 영어와 달리, 한국어는 띄어쓰기 만으로도 단어 토큰을 구분하기 어려움"
      ],
      "id": "imposed-killing"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo7o_jXFrpjf"
      },
      "source": [
        "## 토큰화 중 생기는 선택의 순간\n",
        "\n",
        "- 토큰화 시 예상치 못한 경우가 있어서 토큰화 기준을 세워야 하는 경우가 발생\n",
        "- 해당 데이터가 가지고 어떤 용도로 사용할 것인지에 따라 기준을 확립\n",
        "- 예로, 영어권에서 아포스트로피(')가 들어 있는 단어의 토큰화 문제"
      ],
      "id": "fo7o_jXFrpjf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6T3XFD6sJLD"
      },
      "source": [
        "**NLTK는 영어 코퍼스를 토큰화 하기 위한 도구를 제공**"
      ],
      "id": "I6T3XFD6sJLD"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAnhXiexvDl-",
        "outputId": "07dc96f9-980b-4379-db45-efdfcd89567d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "id": "lAnhXiexvDl-",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib0hD4tLsI9g"
      },
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "id": "Ib0hD4tLsI9g",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mjg2bkE9wRIq"
      },
      "source": [
        "sentence = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\""
      ],
      "id": "Mjg2bkE9wRIq",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGW2J05jrqFo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6403e80-9d8d-402a-a44b-3a523523b65f"
      },
      "source": [
        "print(sentence)\n",
        "print(word_tokenize(sentence))"
      ],
      "id": "TGW2J05jrqFo",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\n",
            "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00ezmR49qsyn"
      },
      "source": [
        "**word_tokenize**\n",
        "- Don't를 Do와 n't로 분리했으나, Jone's는 Jone'과 s로 분리한 것을 확인 "
      ],
      "id": "00ezmR49qsyn"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWmEV2r7v210"
      },
      "source": [
        "from nltk.tokenize import WordPunctTokenizer"
      ],
      "id": "DWmEV2r7v210",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gU8N8bS-rtiq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98e909e4-5cb1-4dd7-9a49-7fbefc597b53"
      },
      "source": [
        "print(WordPunctTokenizer().tokenize(sentence))"
      ],
      "id": "gU8N8bS-rtiq",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97sjXnCWwlBn"
      },
      "source": [
        "**WordPunctTokenizer**\n",
        "- 구두점을 별도로 분류하는 특징\n",
        "- 앞서 확인한 word_tokenize와 달리 Don't를 Don과 '와 t로 분리\n",
        "- Jone's도 Jone과 '와 s로 분리"
      ],
      "id": "97sjXnCWwlBn"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0HCv69yw18q"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
      ],
      "id": "I0HCv69yw18q",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rk5UpJk1xE9B",
        "outputId": "6720d8cb-9ef5-4511-e9be-2a12231a157d"
      },
      "source": [
        "print(text_to_word_sequence(sentence))"
      ],
      "id": "Rk5UpJk1xE9B",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzJa8bOHxVao"
      },
      "source": [
        "**Keras의 text_to_word_sequence**\n",
        "- 기본적으로 알파벳을 모두 소문자화\n",
        "- 마침표나 컴마, 느낌표 등의 구두점 제거\n",
        "- 하지만 Don't나 Jone's같은 아포스트로피는 보존"
      ],
      "id": "dzJa8bOHxVao"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdlC41Z5xwv2"
      },
      "source": [
        "## 토큰화에서 고려해야할 사항\n",
        "---\n",
        "- 토큰화 작업을 단순하게 코퍼스에서 구두점을 제외하고 공백 기준으로 잘라내는 작업이라 간주할 수 없음"
      ],
      "id": "YdlC41Z5xwv2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBr2UoR-yAjs"
      },
      "source": [
        "### 구두점이나 특수 문자를 단순 제외해서는 안된다.\n",
        "---\n",
        "- 코퍼스에 대한 정제 작업 시 구두점 조차 하나의 토큰으로 분류되기도 함\n",
        "  - ex) 마침표(.)의 경우 단어의 경계를 알 수 있는데 도움되므로 마침표를 제거하지 않을 수 있음\n",
        "  - ex) Ph.D, AT&T, $45.55의 달러와 점($ & .) 01/02/06의 슬래시 등은 특정 의미를 나타내므로 하나로 취급해야 하며, 123,456,789처럼 숫자를 세 자리 단위로 표현시에서 필수"
      ],
      "id": "bBr2UoR-yAjs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXR44RcEzBsM"
      },
      "source": [
        "### 줄임말고 단어 내의 띄어쓰기가 있는 경우\n",
        "---\n",
        "- 영어권 언어의 아포스트로피(')는 압축된 단어를 다시 펼치는 역할을 하기도 함\n",
        "  - ex) what're = what are\n",
        "    - 상기 예시에서 re를 접어(clitic)이라고 함\n",
        "  - ex) New York이나 rock 'n' roll은 한 단어지만 중간에 띄어쓰기가 존재함\n",
        "  - 단어 사이에 띄어쓰기가 있어도 하나의 토큰으로 간주해야 함"
      ],
      "id": "bXR44RcEzBsM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9xWdoMQzr6m"
      },
      "source": [
        "### 표준 토큰화 예제\n",
        "---\n",
        "- 표준으로 쓰이고 있는 토큰화 방법 중 하나인 Penn Treebank Tokenization 규칙 및 활용\n",
        "  - 규칙 1 : 하이푼으로 구성된 단어는 하나로 유지\n",
        "  - 규칙 2 : doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리"
      ],
      "id": "m9xWdoMQzr6m"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Rn6HtU70M2u"
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer"
      ],
      "id": "0Rn6HtU70M2u",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbWYJEmn0S4b"
      },
      "source": [
        "tokenizer = TreebankWordTokenizer()\n",
        "text = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\""
      ],
      "id": "qbWYJEmn0S4b",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7XPid__0h6z",
        "outputId": "46f9ef31-533a-4767-f1d5-c369dba44628"
      },
      "source": [
        "print(text)\n",
        "print(tokenizer.tokenize(text))"
      ],
      "id": "j7XPid__0h6z",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\n",
            "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ67Mrns09nU"
      },
      "source": [
        "- 규칙 1에 따라 home-based는 유지\n",
        "- 규칙 2에 따라 doesn't는 does와 n't로 분리"
      ],
      "id": "bZ67Mrns09nU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdr46gSW0l6J",
        "outputId": "f57dd1e9-ff40-48eb-d515-dbc198858ebc"
      },
      "source": [
        "print(sentence)\n",
        "print(tokenizer.tokenize(sentence))"
      ],
      "id": "wdr46gSW0l6J",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\n",
            "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbiSRoEF0ptr"
      },
      "source": [
        "## 문장 토큰화\n",
        "---\n",
        "- 토큰(token)의 단위가 문장\n",
        "- 문장 분류(sentence segmentation)이라고도 함\n",
        "- 보통 갖고 있는 코퍼스가 정제되지 않은 상태라면, 코퍼스가 문장 단위로 구분되어 있지 않을 가능성이 높음\n",
        "- 문장 단위 분류를 ?나 마침표(.)나 ! 기준으로 문장을 잘라낼 수도 있다고 생각하지만, !나 ?는 문장을 구분하기 위한 명확한 구분자(boundary) 역할을 하지만 마침표(.)는 반드시 그렇지는 않음\n",
        "  - ex) Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.\n",
        "  - 단순히 마침표를 구분자로 할 경우 Ph.D의 마침표 전후로도 문장으로 분류됨\n",
        "- 코퍼스가 어떤 국적의 언어인지, 해당 코퍼스 내의 특수문자들이 어떻게 사용되고 있는지에 따라 직접 규칙을 정의해볼 수 있음\n",
        "- 단, 갖고 있는 코퍼스 데이터에 오타나, 문장의 구성 등이 엉망이라면 정해놓은 규칙은 소용없을 수 있음\n",
        "- **NLTK**는 영어 문장의 토큰화를 수행하는 sent_tokenize 지원"
      ],
      "id": "MbiSRoEF0ptr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIAtmsdu1dym"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "id": "GIAtmsdu1dym",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2ZKbkih209j"
      },
      "source": [
        "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\""
      ],
      "id": "q2ZKbkih209j",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxMp--Zf3Bdf",
        "outputId": "e34b6a2c-758b-4585-d9c0-9db34265bffd"
      },
      "source": [
        "print(text)\n",
        "print(sent_tokenize(text))"
      ],
      "id": "qxMp--Zf3Bdf",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\n",
            "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO9SO_913FN7"
      },
      "source": [
        "성공적으로 모든 문장을 구분해냄"
      ],
      "id": "nO9SO_913FN7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG6ina1V3YOg"
      },
      "source": [
        "text = \"I am actively lokking for Ph.D. students. and you are a Ph.D student.\""
      ],
      "id": "nG6ina1V3YOg",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keE7_7ME3fWw",
        "outputId": "eb566ee2-e502-4278-a0fd-d8fb7c0acca2"
      },
      "source": [
        "print(text)\n",
        "print(sent_tokenize(text))"
      ],
      "id": "keE7_7ME3fWw",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I am actively lokking for Ph.D. students. and you are a Ph.D student.\n",
            "['I am actively lokking for Ph.D. students.', 'and you are a Ph.D student.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OSJs8MG3hd_"
      },
      "source": [
        "- NLTK는 단순히 마침표를 구분자로 하여 문장을 구분하지 않았기에, Ph.D.를 문장 내의 단어로 인식 성공\n",
        "- 한국어에 대한 토큰화 도구는, 박상길닝미 개발한 KSS(Korean Sentence Spliiter)를 추천"
      ],
      "id": "2OSJs8MG3hd_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as-Evmvi4BDH"
      },
      "source": [
        "#pip install kss\n",
        "import kss"
      ],
      "id": "as-Evmvi4BDH",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ2AQ1zT4Cvt"
      },
      "source": [
        "text = \"딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어려워요. 농담 아니에요. 이제 해보면 알걸요?\""
      ],
      "id": "QQ2AQ1zT4Cvt",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eMK5g8q4OlA",
        "outputId": "05a5fb49-2713-4f03-a6e7-388163e4e3ce"
      },
      "source": [
        "print(text)\n",
        "print(kss.split_sentences(text))"
      ],
      "id": "2eMK5g8q4OlA",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어려워요. 농담 아니에요. 이제 해보면 알걸요?\n",
            "['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어려워요.', '농담 아니에요.', '이제 해보면 알걸요?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K07NVy194R2u"
      },
      "source": [
        "## 이진 분류기(Binary Classifier)\n",
        "---\n",
        "- 문장 토큰화에서의 예외 사항을 발생시키는 마침표 처리를 위해 입력에 따라 두 개의 클래스로 분류하는 이진 분류기 사용\n",
        "  - 두 개의 클래스\n",
        "    1. 마침표(.)가 단어의 일부분, 즉 마침표가 약어(abbreivation)\n",
        "    2. 마침표(.)가 정말로 문장의 구분자(boundary)일 경우\n",
        "- 마침표가 어떤 클래스에 속하는지 경정하기 위해서는, 어떤 마침표가 주로 약어로 쓰이는지 알아야 함, 그렇기에 이진 분류기 구현 시 약어 사전이 유용\n",
        "  - 약어 사전의 예) https://public.oed.com/how-to-use-the-oed/abbreviations/\n",
        "- 문장 토큰화를 수행하는 오픈 소스\n",
        "  - NLTK, OpenNLP, 스탠포드 CoreNLP, splitta, LingPipe 등\n",
        "  - 문장 토큰화 규칙을 세울 시, 발생할 수 있는 예외 사항을 다룬 참고 자료\n",
        "    - https://www.grammarly.com/blog/engineering/how-to-split-sentences/"
      ],
      "id": "K07NVy194R2u"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjKVmcT75mY4"
      },
      "source": [
        "## 한국어에서의 토큰화의 어려움\n",
        "---\n",
        "- 영어는 New York과 같은 합성어나 he's와 같은 줄임말에 대한 예외처리만 한다면, 띄어쓰기(whitespace)를 기준으로 토큰화해도 잘 작동함\n",
        "- 하지만 한국어에선, 띄어 쓰기 단위가 되는 단위를 '어절'이라고 하는데 어절 토큰화는 한국어 NLP에서 지양\n",
        "- 어절 토큰화 != 단어 토큰화\n",
        "- 근본적인 이유 : 한국어는 영어와 다른 교착어(조사, 어미 등을 붙여서 말을 만드는 언어)"
      ],
      "id": "SjKVmcT75mY4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lGTPSW78-sz"
      },
      "source": [
        "### 한국어는 교착어\n",
        "  - 영어 => 그 : he/him이라는 주어나 목적어\n",
        "  = 한국어 => 그 : 그는, 그와, 그에, 그는, 그를 등 같은 조사가 띄어쓰기 없이 바로 붙음\n",
        "  - 한국어 NLP에서 조사를 분리해야 함\n",
        "  - 즉, 어절에서 조사 등의 무언가가 붙어있는 경우가 많아 이를 전부 분리해야 함\n",
        "---\n",
        "한국어 토큰화에선 형태소(morpheme) 개념이 필수\n",
        "- 형태소 : 자립 형태소와 의존 형태소\n",
        "  - 자립 형태소 : 접사, 어미, 조사와 상관없이 자립할 수 있는 형태소, 그 자체가 단어, 체언(명사,대명사,수사), 수식언(관형사, 부사), 감탄사 등\n",
        "  - 의존 명태소 : 다른 형태소와 결합하여 사용되는 형태소. 접사, 어미, 조사, 어간\n",
        "  - ex) 에디가 딥러닝책을 읽었다\n",
        "  - 자립 형태소 : 에디, 딥러닝책\n",
        "  - 의존 형태소 : -가, -을- 읽-, -었, -다"
      ],
      "id": "2lGTPSW78-sz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT9wUMCr-cM5"
      },
      "source": [
        "### 한국어는 띄어쓰기가 영어보다 잘 지켜지지 않음\n",
        "- 한국어 코퍼스가 뉴스 기사와 같이 띄어쓰기를 잘 지키려고 노력한 글을 제외하면, 영어권 언어와 비교하여 많은 경우 띄어쓰기가 틀렸거나 잘 지켜지지 않음\n",
        "- 한국어는 띄어쓰기를 잘 하지 않아도 글을 쉽게 이해할 수 있느 언어이기 때문문"
      ],
      "id": "nT9wUMCr-cM5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz7yN1lp_AmI"
      },
      "source": [
        "## 품사 태깅(part-of-speech tagging)\n",
        "---\n",
        "- 단어 표기는 같지만 품사에 따라 단어의 의미가 달라지기도 함\n",
        "- 영어 단어 fly : 동사=> 날다 / 명사 => 파리\n",
        "- 한국어 단어 못 : 부사 : 동사의 동작을 할 수 없다는 의미 / 명사 : 목재와 따위를 고정하는 물건\n",
        "- 즉, 단어의 의미를 제대로 파악하기 이ㅜ해선 단어가 어떤 품사로 쓰였는지 파악하는 것이 주요 지표\n",
        "- 단어 토큰화 과정에서 각 단어가 어떤 품사로 쓰였는지를 구분해놓기도 하는데, 이자 작업을 품사 태깅(part-of-speech tagging)\n",
        "- NLTK와 KoNLp에서는 어떤 품사 태깅이 되는지 실습"
      ],
      "id": "zz7yN1lp_AmI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwkJtVqA_mv6"
      },
      "source": [
        "##  NLTK와 KoNLPy를 이용한 영어, 한국어 토큰화 실습\n",
        "- NLTK에서는 영어 코퍼스에 품사 태깅 기능을 지원\n",
        "- NLTK에서는 Penn Treebank POS Tags라는 기준"
      ],
      "id": "OwkJtVqA_mv6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSEv3ayC_yR-"
      },
      "source": [
        "# import nltk\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag"
      ],
      "id": "WSEv3ayC_yR-",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKpRmH7p_29o"
      },
      "source": [
        "text = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\""
      ],
      "id": "VKpRmH7p_29o",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkLl7nVD_7Wa",
        "outputId": "7a283aa5-d811-4d72-c744-1f469cdd01cb"
      },
      "source": [
        "print(text)\n",
        "print(\"*\"*100)\n",
        "print(word_tokenize(text))\n",
        "print(\"*\"*100)\n",
        "pos_tag(word_tokenize(text))"
      ],
      "id": "wkLl7nVD_7Wa",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I am actively looking for Ph.D. students. and you are a Ph.D. student.\n",
            "****************************************************************************************************\n",
            "['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n",
            "****************************************************************************************************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " ('am', 'VBP'),\n",
              " ('actively', 'RB'),\n",
              " ('looking', 'VBG'),\n",
              " ('for', 'IN'),\n",
              " ('Ph.D.', 'NNP'),\n",
              " ('students', 'NNS'),\n",
              " ('.', '.'),\n",
              " ('and', 'CC'),\n",
              " ('you', 'PRP'),\n",
              " ('are', 'VBP'),\n",
              " ('a', 'DT'),\n",
              " ('Ph.D.', 'NNP'),\n",
              " ('student', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV0rnhFyA_aW"
      },
      "source": [
        "Penn Treebank POG Tags에서\n",
        "- PRP는 인칭 대명사\n",
        "- VBP는 동사\n",
        "- RB는 부사\n",
        "- VBG는 현재부사\n",
        "- IN은 전치사\n",
        "- NNP는 고유 명사\n",
        "- NNS는 복수형 명사\n",
        "- CC는 접속사\n",
        "- DT는 관사"
      ],
      "id": "kV0rnhFyA_aW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5Tv7lC4BAZM"
      },
      "source": [
        "---\n",
        "**한국어 자연어 처리(KoNLPy)라는 파이썬 패키지**\n",
        "- 코엔엘파이를 통해서 사용할 수 있는 형태소 분석기로 Okt(Open Korea Text), 메캅(Mecab), 코모란(Komoran), 한나눔(Hannanum), 꼬꼬마(Kkma)\n",
        "- 한국어 NLP에서 형태소 분석기를 사용하는 건 단어 토큰화가 아니라 형태소 단위로 형태소 토큰화 수행\n"
      ],
      "id": "C5Tv7lC4BAZM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hw2cBygaB-jf"
      },
      "source": [
        "# !pip install konlpy\n",
        "from konlpy.tag import Okt"
      ],
      "id": "Hw2cBygaB-jf",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZSoivPDERqE"
      },
      "source": [
        "okt = Okt()\n",
        "text = \"열심히 코딩한 당신, 연휴에는 여행을 가봐요\""
      ],
      "id": "GZSoivPDERqE",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03wJ-DjcEXsD",
        "outputId": "c5f06e8a-6239-48ac-f2ad-f9c954cfc360"
      },
      "source": [
        "print(text)\n",
        "print(\"*\"*100)\n",
        "print(okt.morphs(text))\n",
        "print(\"*\"*100)\n",
        "print(okt.pos(text))\n",
        "print(\"*\"*100)\n",
        "print(okt.nouns(text))"
      ],
      "id": "03wJ-DjcEXsD",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "열심히 코딩한 당신, 연휴에는 여행을 가봐요\n",
            "****************************************************************************************************\n",
            "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n",
            "****************************************************************************************************\n",
            "[('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n",
            "****************************************************************************************************\n",
            "['코딩', '당신', '연휴', '여행']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kii6wRwNEoes"
      },
      "source": [
        "1. morphs : 형태소 추출\n",
        "2. pos : 품사 태깅(part-of-speech tagging)\n",
        "3. nouns : 명사 추출"
      ],
      "id": "kii6wRwNEoes"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydVl2_EhE8qi"
      },
      "source": [
        "from konlpy.tag import Kkma\n",
        "kkma = Kkma()"
      ],
      "id": "ydVl2_EhE8qi",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVsiLUVnFFve",
        "outputId": "64152dbe-23ef-492a-972a-49fa47e27c4d"
      },
      "source": [
        "print(text)\n",
        "print(\"*\"*100)\n",
        "print(kkma.morphs(text))\n",
        "print(\"*\"*100)\n",
        "print(kkma.pos(text))\n",
        "print(\"*\"*100)\n",
        "print(kkma.nouns(text))"
      ],
      "id": "RVsiLUVnFFve",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "열심히 코딩한 당신, 연휴에는 여행을 가봐요\n",
            "****************************************************************************************************\n",
            "['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n",
            "****************************************************************************************************\n",
            "[('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n",
            "****************************************************************************************************\n",
            "['코딩', '당신', '연휴', '여행']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnK10ZMJFO1j"
      },
      "source": [
        "- Okt 형태소와는 다른 결과\n",
        "- 각 형태소마다 성능이 다르기에, 형태소 분석기의 선택은 사용하고자 하는 용도에 어떤 형태소 분석기가 적절한지 판단하고 사용\n",
        "- 예로, 속도를 중요시할 경우 메캅\n",
        "- 한국어 형태소 분석기 성능 비교\n",
        "  https://iostream.tistory.com/144\n",
        "http://www.engear.net/wp/%ED%95%9C%EA%B8%80-%ED%98%95%ED%83%9C%EC%86%8C-%EB%B6%84%EC%84%9D%EA%B8%B0-%EB%B9%84%EA%B5%90/"
      ],
      "id": "xnK10ZMJFO1j"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlL5KIB8Ft3X"
      },
      "source": [
        "# !git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "# %cd Mecab-ko-for-Google-Colab\n",
        "# !bash install_mecab-ko_on_colab190912.sh"
      ],
      "id": "zlL5KIB8Ft3X",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ_oGZ6VY_fm"
      },
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()"
      ],
      "id": "CJ_oGZ6VY_fm",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlqFcL-9ZB5m",
        "outputId": "877c1cdb-3c83-4edc-a1ab-7ebfe99cce99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(text)\n",
        "print(\"*\"*100)\n",
        "print(mecab.morphs(text))\n",
        "print(\"*\"*100)\n",
        "print(mecab.pos(text))\n",
        "print(\"*\"*100)\n",
        "print(mecab.nouns(text))"
      ],
      "id": "zlqFcL-9ZB5m",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "열심히 코딩한 당신, 연휴에는 여행을 가봐요\n",
            "****************************************************************************************************\n",
            "['열심히', '코딩', '한', '당신', ',', '연휴', '에', '는', '여행', '을', '가', '봐요']\n",
            "****************************************************************************************************\n",
            "[('열심히', 'MAG'), ('코딩', 'NNG'), ('한', 'XSA+ETM'), ('당신', 'NP'), (',', 'SC'), ('연휴', 'NNG'), ('에', 'JKB'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가', 'VV'), ('봐요', 'EC+VX+EC')]\n",
            "****************************************************************************************************\n",
            "['코딩', '당신', '연휴', '여행']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}