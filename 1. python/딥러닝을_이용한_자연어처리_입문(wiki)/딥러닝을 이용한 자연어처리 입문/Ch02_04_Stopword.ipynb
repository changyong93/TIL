{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 불용어\n",
    "- corpus 내에서 큰 의미 없는 토큰을 제거해주는 작업이 필요\n",
    "  - i, my, me, over, and 등\n",
    "- 직접 정의할 수도 있고 패키지 내에서 정의한 불용어도 사용할 수 있음\n",
    "- 단, 불용어 처리는 task에 따라 상이하며, 그 예로, 대화체에서는 불용어를 처리하면 의미가 달라지므로 제거하면 안됨"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NLTK와 spcay 불용어 확인"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(f\"[len: {len(stopwords.words('english'))} || 10 stopwords in nltk]\",\"\\n\", stopwords.words(\"english\")[:10])\n",
    "print(f\"[len: {len(en.Defaults.stop_words)} || 10 stopwords in spacy]\",\"\\n\", list(en.Defaults.stop_words)[:10])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[len: 179 || 10 stopwords in nltk] \n",
      " ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
      "[len: 326 || 10 stopwords in spacy] \n",
      " ['herself', 'and', 'eleven', 'over', 'often', 'any', 'own', 'also', 'fifty', 'say']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. NLTK와 spacy를 통해서 불용어 제거하기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "\n",
    "example = \"Family is not an important thing. It's everything.\"\n",
    "\n",
    "nl = word_tokenize\n",
    "nl_stopwords = stopwords.words(\"english\")\n",
    "\n",
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nl_res = [word for word in nl(example) if word not in nl_stopwords]\n",
    "sp_res = [word for word in sp(example) if word.is_stop == False]\n",
    "\n",
    "print(\"[example]\",\"\\n\", example)\n",
    "print(\"[nltk_result]\",\"\\n\", nl_res) # is, not, an 제거\n",
    "print(\"[spacy_result]\",\"\\n\", sp_res) # is, not, an, It 's 제거"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[example] \n",
      " Family is not an important thing. It's everything.\n",
      "[nltk_result] \n",
      " ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
      "[spacy_result] \n",
      " [Family, important, thing, ., .]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. 한국어에서 불용어 제거하기\n",
    "- 한국어에서 불용어를 제거하는 방법으로는 간단하게는 토큰화 후에 조사, 접속사 등을 제거\n",
    "-  하지만 불용어를 제거하려고 하다보면 조사나 접속사와 같은 단어들뿐만 아니라 명사, 형용사와 같은 단어들 중에서 불용어로서 제거하고 싶은 단어들이 존재\n",
    "- 결국에는 사용자가 직접 불용어 사전을 만들게 되는 경우도 많음\n",
    "- 아래의 링크는 보편적으로 선택할 수 있는 한국어 불용어 리스트 자료 (여전히 절대적인 기준은 아닙니다.)\n",
    "  - 링크 : https://www.ranks.nl/stopwords/korean"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
    "stop_words = \"아무거나 아무렇게나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 하면 아니거든\"\n",
    "# 위의 불용어는 명사가 아닌 단어 중에서 저자가 임의로 선정한 것으로 실제 의미있는 선정 기준이 아님\n",
    "stop_words=stop_words.split(' ')\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "result = [] \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        result.append(w) \n",
    "# 위의 4줄은 아래의 한 줄로 대체 가능\n",
    "# result=[word for word in word_tokens if not word in stop_words]\n",
    "\n",
    "print(word_tokens) \n",
    "print(result)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['고기를', '아무렇게나', '구우려고', '하면', '안', '돼', '.', '고기라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n",
      "['고기를', '구우려고', '안', '돼', '.', '고기라고', '다', '같은', '게', '.', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}