{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "다음 자료는 유원준님의 위키독스 딥러닝을 이용한 자연어 처리 입문을 참고하여 작성한 내용입니다.\n",
    "- https://wikidocs.net/21698"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 토큰화(Tokenization)\n",
    "- 수집한 raw 데이터에 대해 전처리가 되지 않은 상태인 경우 토큰화(tokenization) & 정제(cleaning) & 정규화(normalization) 필요\n",
    "- 코퍼스(corpus)에서 토큰(token)이라 불리는 단위로 나누는 작업을 토큰화(tokenization)\n",
    "- 토큰 단위는 상황에 따라 다르지만, 위한 의미있는 단위로 토큰을 정의"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 단어 토큰화(word tokenization)\n",
    "- 토큰을 단어 단위로 정의한 경우 단어 토큰화라고 함\n",
    "- 단어 토큰은, 단어 외에도 의미를 갖는 문자열로도 간주(단어구)\n",
    "- 따라서 아포스트로피나, 구두점, 특수문자의 정제 방법에 따라 단어의 의미가 달라지는데, 이는 토크나이저별 규칙에 따라 다르게 토큰화가 진행됨"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "spacy.__version__"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'3.1.2'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import spacy\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# en_core_web_sm를 다운하지 않은 경우 다운로드\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "#nltk 사용을 위해 punkt 다운로드\n",
    "# import nltk\n",
    "# nltk.download('punkt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") #https://spacy.io/models/en\n",
    "\n",
    "def get_token(text):\n",
    "    spacy_tokenizer = [token.text for token in nlp(text)]\n",
    "    word_tokenizer = word_tokenize(text)\n",
    "    word_puncttokenizer = WordPunctTokenizer().tokenize(text)\n",
    "    word_treebacktokenizer = TreebankWordTokenizer().tokenize(text)\n",
    "\n",
    "    print(\"[text]\", \"\\n\",text, \"\\n\")\n",
    "    print(\"[spacy_token]\", \"\\n\", spacy_tokenizer, \"\\n\")\n",
    "    print(\"[word_token]\", \"\\n\", word_tokenizer, \"\\n\")\n",
    "    print(\"[word_punkt_token]\", \"\\n\", word_puncttokenizer,\"\\n\")\n",
    "    print(\"[word_treeback_token]\", \"\\n\", word_treebacktokenizer,\"\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "아래 경우 두 가지 경우를 살펴보자(밑에 추가 설명)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "text = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"\n",
    "get_token(text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[text] \n",
      " Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop. \n",
      "\n",
      "[spacy_token] \n",
      " ['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.'] \n",
      "\n",
      "[word_token] \n",
      " ['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.'] \n",
      "\n",
      "[word_punkt_token] \n",
      " ['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.'] \n",
      "\n",
      "[word_treeback_token] \n",
      " ['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.'] \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "text=\"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
    "get_token(text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[text] \n",
      " Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own. \n",
      "\n",
      "[spacy_token] \n",
      " ['Starting', 'a', 'home', '-', 'based', 'restaurant', 'may', 'be', 'an', 'ideal', '.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.'] \n",
      "\n",
      "[word_token] \n",
      " ['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal', '.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.'] \n",
      "\n",
      "[word_punkt_token] \n",
      " ['Starting', 'a', 'home', '-', 'based', 'restaurant', 'may', 'be', 'an', 'ideal', '.', 'it', 'doesn', \"'\", 't', 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.'] \n",
      "\n",
      "[word_treeback_token] \n",
      " ['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.'] \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "아포스트로피('), 하이푼(-), 단어 자체의 구두점(Ph.D), 문장의 마침표(.), 달러($) 등 다양한 특수문자가 존재하는데, 경우 따라 해당 특수 문자를 지우거나, 혹은 떼어내는 등의 경우 의미 자체가 달라지게 된다.\n",
    "따라서 각 토크나이저마다 그 규칙이 다른데, 각 경우를 살펴보고 텍스트에 가장 적합한 토크나이저 선정이 중요하다"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 문장 토큰화(sentence tokenization)\n",
    "- 문장 토큰화는 sentence segmentation 으로도 불림\n",
    "- 정제되지 않은 코퍼스는, 문장 단위로 구분되어 있지 않을 가능성이 크다. 따라서 이를 사용하기 위해 적절하게 문장 토큰화가 피룡하다.\n",
    "- 간단히 생각해보면, !, ?, . 등의 특수문자를 기준으로 문장을 나누면 되지만, \"\" 사이에 대화내용을 담은 경우, Ph.D 사이의 구두점, 구어체에서 !!!! 등을 연속으로 사용한 경우 등 다양한 특이케이스가 존재한다. 따라서 단순히 이 규칙을 통해 나누면 안된다.\n",
    "- nltk에서 이러한 sent_tokenizer를 제공하며, spcay에서도 간단히 지원을 한다.\n",
    "- 단, spcay는 cumstomized sentence tokenizer도 생성할 수 있으므로 추후에 공부해보자"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import spacy\n",
    "\n",
    "def get_sentence(text):\n",
    "    spcay_sents = [s for s in nlp(text).sents]\n",
    "    nltk_sents = sent_tokenize(text)\n",
    "\n",
    "    print(\"[text]\",\"\\n\",text,\"\\n\")\n",
    "    print(\"[spcay]\",\"\\n\",spcay_sents,\"\\n\")\n",
    "    print(\"[nltk]\",\"\\n\",nltk_sents,\"\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "text=\"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
    "get_sentence(text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[text] \n",
      " His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near. \n",
      "\n",
      "[spcay] \n",
      " [His barber kept his word., But keeping such a huge secret to himself was driving him crazy., Finally, the barber went up a mountain and almost to the edge of a cliff., He dug a hole in the midst of some reeds., He looked about, to make sure no one was near.] \n",
      "\n",
      "[nltk] \n",
      " ['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.'] \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "text=\"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
    "get_sentence(text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[text] \n",
      " I am actively looking for Ph.D. students. and you are a Ph.D student. \n",
      "\n",
      "[spcay] \n",
      " [I am actively looking for Ph.D. students., and you are a Ph.D student.] \n",
      "\n",
      "[nltk] \n",
      " ['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.'] \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "한국어의 경우 박상길님이 만드신 KSS(Korean Sentence Splitter)가 있으며, 유원준 강사님이 추천하신 도구이다!!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "#kss 설치가 안되어 있다면\n",
    "# !pip install kss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "import kss\n",
    "kss_sent_tokenizer = kss.split_sentences\n",
    "text='딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어려워요. 농담아니에요. 이제 해보면 알걸요?'\n",
    "\n",
    "kss_sents = kss_sent_tokenizer(text)\n",
    "print(\"[text]\",\"\\n\", text,\"\\n\")\n",
    "print(\"[kss_sents]\",\"\\n\", kss_sents,\"\\n\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[text] \n",
      " 딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어려워요. 농담아니에요. 이제 해보면 알걸요? \n",
      "\n",
      "[kss_sents] \n",
      " ['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어려워요. 농담아니에요.', '이제 해보면 알걸요?'] \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 이진 분류기\n",
    "- 문장 토큰화의 예외 사항을 발생시키는 마침표 처리를 위해 이진분류기를 사용하기도 한다.\n",
    "- 여기서 분류하는 biary class로는\n",
    "  - 마침표(.)가 단어의 일부인 경우 : abbreviation, 약어\n",
    "  - 마침표(.)가 문장의 구분자인 경우 : boundary, 구분자\n",
    "\n",
    "- 영어권의 경우 아래 링크의 약어 사전을 참고\n",
    "  - https://public.oed.com/how-to-use-the-oed/abbreviations/\n",
    "\n",
    "- 문장 토큰화의 오픈 소스로, NLTK, OpenNLP, standford CoreNLP, splitta, LingPipe, University of Illinois Sentence Segmentation tool 등이 있으며 자세한 내용은 아래 링크 참고\n",
    "- 링크를 보면 MASC dataset과 OntoNotes dataset으로 각 sent tokenizer에 대한 성능을 볼 수 있다.\n",
    "  - https://www.grammarly.com/blog/engineering/how-to-split-sentences/\n",
    "  - 참고로 OntoNotes를 기반으로 SOTA 모델은!? : https://paperswithcode.com/dataset/ontonotes-5-0"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 한국어 토큰화의 어려움\n",
    "- 한국어는 조사, 어미 등을 붙여서 말을 만드는 교착어라 영어와 달리 띄어쓰기 단위로만 단어 토큰화 진행이 지양\n",
    "- 한국어는 띄어쓰기 규정이 지켜지지 않거나 틀린 코퍼스가 많으므로, 이 점을 유의해야 함\n",
    "\n",
    "### 한국어 == 교착어\n",
    "- 영어: he \\\\ 한국어: 그는, 그와, 그의, 그 등 다양한 어미가 붙음\n",
    "- 즉 한국어는 독립적인 단어로 구성된 것이 아니므로, 조사 등 무언가 붙은 것을 분리 해줘야 함\n",
    "- 한국어 토큰화 == 형태소 단위\n",
    "  - 형태소(morpheme) : 가장 작은 말의 단위\n",
    "  - 자립 형태소 : 접사, 어미, 조사와 상관없이 자립하여 사용할 수 있는 형태소. 그 자체로 단어\n",
    "    -  체언(명사, 대명사, 수사), 수식언(관형사, 부사), 감탄사 등\n",
    "  - 의존 형태소 : 다른 형태소와 결합하여 사용되는 형태소. => 접사, 어미, 조사, 어간"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 품사 태깅(Part-of-speech Tagging)\n",
    "- 같은 단어라도 품사에 따라 의미가 달라짐\n",
    "- 단어를 토큰화 하는 과정에서 각 단어가 어떤 품사인지 구분하는 작업을 POS tagging이라고 함\n",
    "- nltk, konlpy, spcay를 활용하여 진행해볼 예정\n",
    "- 태깅은 패키지마다 다르므로 이 점 유의하며, 각 규칙을 확인해야 함"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "import spacy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "def get_pos(text):\n",
    "    spcay_tokens = [(token, token.pos_) for token in nlp(text)]\n",
    "    nltk_tokens = word_tokenize(text)\n",
    "    nltk_pos = pos_tag(nltk_tokens)\n",
    "\n",
    "    print(\"[text]\",\"\\n\",text,\"\\n\")\n",
    "    print(\"[spcay]\",\"\\n\",spcay_tokens,\"\\n\")\n",
    "    print(\"[nltk]\",\"\\n\",nltk_pos,\"\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "text=\"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
    "get_pos(text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[text] \n",
      " I am actively looking for Ph.D. students. and you are a Ph.D. student. \n",
      "\n",
      "[spcay] \n",
      " [(I, 'PRON'), (am, 'AUX'), (actively, 'ADV'), (looking, 'VERB'), (for, 'ADP'), (Ph.D., 'NOUN'), (students, 'NOUN'), (., 'PUNCT'), (and, 'CCONJ'), (you, 'PRON'), (are, 'AUX'), (a, 'DET'), (Ph.D., 'NOUN'), (student, 'NOUN'), (., 'PUNCT')] \n",
      "\n",
      "[nltk] \n",
      " [('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')] \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "한국어 자연어 처리를 위해서는 KoNLPy(\"코엔엘파이\"라고 읽습니다)라는 파이썬 패키지를 사용할 수 있습니다.\n",
    "코엔엘파이를 통해서 사용할 수 있는 형태소 분석기로 Okt(Open Korea Text), 메캅(Mecab), 코모란(Komoran), 한나눔(Hannanum), 꼬꼬마(Kkma)가 있습니다.\n",
    "\n",
    "한국어 NLP에서 형태소 분석기를 사용한다는 것은 단어 토큰화가 아니라 정확히는 형태소(morpheme) 단위로 형태소 토큰화(morpheme tokenization)를 수행하게 됨을 뜻합니다. 여기선 이 중에서 Okt와 꼬꼬마를 통해서 토큰화를 수행해보도록 하겠습니다. (Okt는 기존에는 Twitter라는 이름을 갖고있었으나 0.5.0 버전부터 이름이 변경되어 인터넷에는 아직 Twitter로 많이 알려져있으므로 학습 시 참고바랍니다.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from konlpy.tag import Mecab, Kkma, Okt, Komoran, Hannanum\n",
    "# mecab 설치 https://velog.io/@changyong93/Ubuntu-18.04-konlpy-mecab-install"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "text = \"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"\n",
    "mecab = Mecab().pos(text)\n",
    "kkma = Kkma().pos(text)\n",
    "okt = Okt().pos(text,stem=True)\n",
    "komoran = Komoran().pos(text)\n",
    "hannanum = Hannanum().pos(text)\n",
    "# morphs, nouns, pos 등의 기능이 있으며,\n",
    "# Okt의 경우 stemming attribute도 있음\n",
    "\n",
    "print(\"[text]\",\"\\n\",text,\"\\n\")\n",
    "print(\"[mecab]\",\"\\n\",mecab,\"\\n\")\n",
    "print(\"[kkma]\",\"\\n\",kkma,\"\\n\")\n",
    "print(\"[okt]\",\"\\n\",okt,\"\\n\")\n",
    "print(\"[hannanum]\",\"\\n\",hannanum,\"\\n\")\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[text] \n",
      " 열심히 코딩한 당신, 연휴에는 여행을 가봐요 \n",
      "\n",
      "[mecab] \n",
      " [('열심히', 'MAG'), ('코딩', 'NNG'), ('한', 'XSA+ETM'), ('당신', 'NP'), (',', 'SC'), ('연휴', 'NNG'), ('에', 'JKB'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가', 'VV'), ('봐요', 'EC+VX+EC')] \n",
      "\n",
      "[kkma] \n",
      " [('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')] \n",
      "\n",
      "[okt] \n",
      " [('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가보다', 'Verb')] \n",
      "\n",
      "[hannanum] \n",
      " [('열심히', 'M'), ('코딩', 'N'), ('하', 'X'), ('ㄴ', 'E'), ('당신', 'N'), (',', 'S'), ('연휴', 'N'), ('에는', 'J'), ('여행', 'N'), ('을', 'J'), ('가', 'P'), ('아', 'E'), ('보', 'P'), ('아', 'E')] \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}