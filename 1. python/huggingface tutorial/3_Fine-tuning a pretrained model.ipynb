{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Processing the data\r\n",
    "이전 chapter에 이어서, 이번엔 특정 task를 위한 classifier를 학습시키는 방법에 대해 알아보자!\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "#same as before\r\n",
    "import torch\r\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\r\n",
    "\r\n",
    "checkpoint = \"bert-base-uncased\"\r\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\r\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\r\n",
    "\r\n",
    "sequences = [\r\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\r\n",
    "    \"This course is amazing!\",\r\n",
    "]\r\n",
    "\r\n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "#new!\r\n",
    "batch[\"labels\"] = torch.tensor([1,1])\r\n",
    "\r\n",
    "optimizer = AdamW(model.parameters())\r\n",
    "loss = model(**batch).loss\r\n",
    "loss.backward()\r\n",
    "optimizer.step()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "과정은 다음과 같지만, fine-tune을 위해선 더 큰 데이셋이 필요!   \r\n",
    "이 section에서는 MRPC(Microsoft Research Paraphrase Corpus) dataset을 사용하여 fine-tune을 할 예정이다.   \r\n",
    "MRPC dataset은 5801쌍의 문장으로 구성되어, 두 문장이 같은 의미인지를 파악하는 테스크에 대한 데이터셋이며, 이 데이터셋은 크기가 작아 실습에 적절하기에 선택하였다고 함."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading a dataset from the Hub\r\n",
    "- hub에는 모델뿐만 아니라 여러 언어로 된 데이터셋도 존재\r\n",
    "- [dataset](https://huggingface.co/datasets), 여기에서 로드 후 처리하는 것이 좋음\r\n",
    "- MPRC는 GLUE benchmark 중 하나의 데이터셋으로, 10개의 서로 다른 텍스트 분류 작업에서 ML 성능을 측정하는데 사용!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "from datasets import load_dataset\r\n",
    "raw_datasets = load_dataset(\"glue\",\"mrpc\")\r\n",
    "raw_datasets"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 28.8kB [00:00, 5.78MB/s]                   \n",
      "Downloading: 28.7kB [00:00, 14.3MB/s]                   \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading and preparing dataset glue/mrpc (download: 1.43 MiB, generated: 1.43 MiB, post-processed: Unknown size, total: 2.85 MiB) to C:\\Users\\ChangYong\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 6.22kB [00:00, 3.13MB/s]\n",
      "Downloading: 1.05MB [00:00, 1.47MB/s]\n",
      "Downloading: 441kB [00:00, 794kB/s]\n",
      "100%|██████████| 3/3 [00:06<00:00,  2.05s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset glue downloaded and prepared to C:\\Users\\ChangYong\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 81.11it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "mprc datasetdict을 보면 \r\n",
    "- 3개 set : training, validation, test\r\n",
    "- 각 set 구성 : sen1, sen2, label, idx\r\n",
    "- 각 set은 num_rows 만큼 sent-pairs가 존재"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "raw_train_dataset = raw_datasets[\"train\"]\r\n",
    "raw_train_dataset[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "각 row별로 정수가 부여되어 있고, idx가 row의 정수번호이다   \r\n",
    "dataset features는 다음과 같다   \r\n",
    "classLabel이 0이면 not_equivalent, 1이면 equivalent"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "raw_train_dataset.features"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing a dataset\r\n",
    "- 데이터셋 전처리는, 기존과 같이 텍스트를 모델 inputs에 맞게, 숫자로 변환해야 함\r\n",
    "- 문장 쌍의 첫번째와 두번를 다음과 같이 직접 토큰화 가능"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "tokenized_sent1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"], padding=True, truncation=True, return_tensors=\"pt\")\r\n",
    "tokenized_sent2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"], padding=True, truncation=True, return_tensors=\"pt\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "하지만 이렇게 처리를 하면, 두 문장이 관련 있는지 없는지를 예측할 수 없음   \r\n",
    "두 시퀀스를 쌍으로 처리하고 적절한 전처리를 해야 함   \r\n",
    "다행히, 토크나이저는 한 쌍의 시퀀스를 처리가능\r\n",
    "\r\n",
    "input_type_ids는 해당 문장이 첫번째 seq인지 두번째seq인지를 구분하는 값!\r\n",
    "\r\n",
    "다만, checkpoint에 따라 token_type_ids 는 불필요 할 수도 있음\r\n",
    "- e.g.) distilBERT => They are only returned when the model will know what to do with them, because it has seen them during its pretraining.\r\n",
    "\r\n",
    "일반적으로 model과 tokenizer는 같은 checkpoint를 사용하기에  부분은 고려안해도 됨"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\r\n",
    "print(inputs)\r\n",
    "\r\n",
    "print(\"\\n\",tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      " ['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "이제 토크나이저가 한 쌍의 문장을 어떻게 다루는지 보았으므로, 실제 tokenized_dataset을 생성해보자"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenized_dataset = tokenizer(raw_datasets[\"train\"][\"sentence1\"],\r\n",
    "                              raw_datasets[\"train\"][\"sentence2\"],\r\n",
    "                              padding=True,\r\n",
    "                              truncation=True,\r\n",
    "                              return_tensors=\"pt\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 이 방법도 잘 잘독하지만, 메모리 리소스 제한으로 인해 쉽지 않음...   \r\n",
    "- data를 dataset 형태로 유지하기 위해 dataset.map 방법을 사용해볼 예정\r\n",
    "- 이 방법은 tokenization뿐만 아니라 더 많은 전처리 task도 수행할 수 있는 만큼 작업의 유연성이 증가됨!\r\n",
    "우선 map함수를 위해 tokenize function을 정의해보자"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "def tokenize_function(example):\r\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 이 함수는 dataset의 items와 같이 dictionary에서 값을 받아서 input_ids, attention_mask, token_type_ids이 key로 된 값들을 반환!\r\n",
    "- 그리고 map 함수를 쓸 때 bathched=True를 사용하면 tokenization 속도를 높일 수 있음\r\n",
    "- 그리고 현재 토크나이저에 padding을 제외했는데, 이는 배치별로 패디을 진행하여 최대 길이가 아니라 배치의 최대 길이로 패딩을 진행\r\n",
    "- 이렇게 하면 배치별로 길이가 매우 다르기에, 많은 시간과 소스를 절약할 수 있음!\r\n",
    "- 단, 만능은 아님. 만약 seq 길이별 비슷한 의미를 가지고 있다고 가정한다면, 각 배치별로는 비슷한 의미끼리 묶이지만, 다른 배치와는 의미가 다를 수 있음. 따라서 이 경우 모델 성능 저하를 유발 할 수 있음!\r\n",
    "- 여하튼, Batched=True를 사용하여 batched별로 함수가 적용됨"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\r\n",
    "tokenized_datasets"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 16.46ba/s]\n",
      "Loading cached processed dataset at C:\\Users\\ChangYong\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-54bdf54eb2bd302f.arrow\n",
      "Loading cached processed dataset at C:\\Users\\ChangYong\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-83846288736966a5.arrow\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Num_proc argument로 process 개수를 지정 시 multiprocessing이 가능!\r\n",
    "- 다만, 해당 토크나이저 라이브러리에서는 이미 빠른 샘플 처리를 위해 여러 스레드를 사용\r\n",
    "- 따라서 이 라이브러리에서 지원하는 fast tokenizer를 사용하지 않을 경우 이 옵션을 사용하면 보다 빠른 처리가 가능\r\n",
    "---\r\n",
    "- tokenize_function은 input_ids, attention_mask, token_type_ids가 있는 dict를 반환\r\n",
    "- 앞으로 진행해볼, 위에서 말한 것처럼, batch별로 패딩을 하는 것을 dynamic padding이라고 함"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dynamic padding\r\n",
    "- PyTorch에서는 배치 내의 data sample을 전처리하는 collate 함수가 있음\r\n",
    "- 이것은 dataloader를 만들 때 전달할 수 있는 argument\r\n",
    "- default는 샘플들을 torch tensor로 변환하고 연결해주는 역할\r\n",
    "- 다만 여기선, 입력값의 크기가 모두 같지 않기에 사용안함\r\n",
    "- 패딩을 위한 collate 함수는 transformers 라이브러리에서 지원!!\r\n",
    "  - **DataCollatorWithPadding**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "from transformers import DataCollatorWithPadding\r\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\r\n",
    "data_collator"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataCollatorWithPadding(tokenizer=PreTrainedTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}), padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')"
      ]
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "데스트를 위해 training set 일부만 가져와서 살펴보자"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "samples = tokenized_datasets[\"train\"][:8]\r\n",
    "samples = {k:v for k, v in samples.items()\\\r\n",
    "           if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\r\n",
    "[len(x) for x in samples[\"input_ids\"]]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[50, 59, 47, 67, 59, 50, 62, 32]"
      ]
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "samples의 seq 길이는 다양하며, 동적 패딩이 없으면 모두 seq_len=67만큼 padding이 되지만, 동적패딩을 하면 batch별 최대 길이로 패딩이 진행된다. data_collator에서 배치에 동적으로 패딩되는지 다시 확인해보자"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "source": [
    "batch = data_collator(samples)\r\n",
    "{k: v.shape for k, v in batch.items()}"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'attention_mask': torch.Size([8, 67]),\n",
       " 'input_ids': torch.Size([8, 67]),\n",
       " 'token_type_ids': torch.Size([8, 67]),\n",
       " 'labels': torch.Size([8])}"
      ]
     },
     "metadata": {},
     "execution_count": 115
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fine-tuning a model with the Trainer API\r\n",
    "- transformers에서는 fine-tune을 위한 traniner class를 지원!!\r\n",
    "- 데이터 전처리를 끝냈다면, Trainer를 define하기 위해 몇 가지만 남은 상태!!\r\n",
    "\r\n",
    "이번 section에선 지난 section을 이어서 실행시킬 것!!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "source": [
    "from datasets import load_dataset\r\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\r\n",
    "\r\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\r\n",
    "checkpoint = \"bert-base-uncased\"\r\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\r\n",
    "\r\n",
    "def tokenize_function(example):\r\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\r\n",
    "\r\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\r\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset glue (C:\\Users\\ChangYong\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 599.96it/s]\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\ChangYong/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\ChangYong/.cache\\huggingface\\transformers\\45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\ChangYong/.cache\\huggingface\\transformers\\534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\ChangYong/.cache\\huggingface\\transformers\\c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\ChangYong/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at C:\\Users\\ChangYong\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-2ff2f76fa35622cd.arrow\n",
      "Loading cached processed dataset at C:\\Users\\ChangYong\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-32e225bb550d8c33.arrow\n",
      "\n",
      "100%|██████████| 2/2 [00:00<00:00, 20.82ba/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training\r\n",
    "- Trainer를 define하기 위해 남은 첫번째는 학습 및 평가에 활용할 하이퍼파라미터가 포함된 TrainingArguments를 define하는 것!\r\n",
    "- 지정할 argument는 모델이 저장될 디렉토리와 도중에 저장될 checkpoint의 위치!!\r\n",
    "- 나머지는 기본으로 둘 것이며, 이게 기본적인 fine-tuning에 매우 적합하다고 함\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "source": [
    "from transformers import TrainingArguments\r\n",
    "training_args = TrainingArguments(\"test-trainer\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "source": [
    "training_args"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=0,\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_pin_memory=True,\n",
       "ddp_find_unused_parameters=None,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "do_eval=False,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_steps=None,\n",
       "evaluation_strategy=IntervalStrategy.NO,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "gradient_accumulation_steps=1,\n",
       "greater_is_better=None,\n",
       "group_by_length=False,\n",
       "ignore_data_skip=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=5e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=False,\n",
       "local_rank=-1,\n",
       "log_level=-1,\n",
       "log_level_replica=-1,\n",
       "log_on_each_node=True,\n",
       "logging_dir=test-trainer\\runs\\Sep27_00-39-41_DESKTOP-FJL0CIS,\n",
       "logging_first_step=False,\n",
       "logging_steps=500,\n",
       "logging_strategy=IntervalStrategy.STEPS,\n",
       "lr_scheduler_type=SchedulerType.LINEAR,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=None,\n",
       "mp_parameters=,\n",
       "no_cuda=False,\n",
       "num_train_epochs=3.0,\n",
       "output_dir=test-trainer,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=8,\n",
       "per_device_train_batch_size=8,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=test-trainer,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "remove_unused_columns=True,\n",
       "report_to=['tensorboard'],\n",
       "resume_from_checkpoint=None,\n",
       "run_name=test-trainer,\n",
       "save_on_each_node=False,\n",
       "save_steps=500,\n",
       "save_strategy=IntervalStrategy.STEPS,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "sharded_ddp=[],\n",
       "skip_memory_metrics=True,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_legacy_prediction_loop=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.0,\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 163
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "다음 step은 모델을 정의"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "source": [
    "from transformers import AutoModelForSequenceClassification\r\n",
    "\r\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\ChangYong/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\ChangYong/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 아래 경고문은, BERT는 문장 쌍에 대해 pre-trained 되어 있지 않으므로 classifier가 새로운 것으로 지정되는 것\r\n",
    "- 그리고 그에 따라 일부 가중치도 초기화가 되므로, down-stream task를 위해 추가로 학습을 권장하는 내용\r\n",
    "\r\n",
    "이제 모델이 확보되었으면, model, training_args, training & validation datsets, data_collator, tokenizer를 가지고 Trainer를 정의하자"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "source": [
    "from transformers import Trainer\r\n",
    "trainer = Trainer(\r\n",
    "    model, #model\r\n",
    "    training_args, #Trainer의 arguments\r\n",
    "    train_dataset = tokenized_datasets[\"train\"], #토큰화만 수행된 데이터셋\r\n",
    "    eval_dataset = tokenized_datasets[\"validation\"], #토큰화만 수행된 데이터셋\r\n",
    "    data_collator=data_collator, #batch별 padding\r\n",
    "    tokenizer=tokenizer, #토크나이저\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "이제 fine-tune은 trainer로 학습을 진행하면 끝!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainer.train()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 이렇게 하면 finetune이 시작되고 500 step마다 loss에 대한 log값을 출력해줌\r\n",
    "- 그러나 다음과 같은 이유로 모델이 잘 수행되는지 확인이 불가\r\n",
    "  - Trainer를 선언할 때 evaluation_strategy를 epoch or steps으로 지정을 안함\r\n",
    "  - evaluation을 위한 compute_metrics를 제공하지 않음!\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation\r\n",
    "- 다시,!! compute_metrics 함수를 구현하고 trainer에 어떻게 사용할지 알아보자\r\n",
    "- 우선 compute_metrics은 trainer의 EvalPrediction object를 사용"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\r\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 408\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(408, 2) (408,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- predict의 output은 predictions, label_ids, metrics로 된 name_tuple\r\n",
    "- metrics는 loss와 runtime이 포함되어 있음\r\n",
    "- compute_metrics에 predictions을 포함하면 반환되는 metric의 하나로 포함됨!\r\n",
    "- 보다시피 예측은 (408,2), label은 (408,)로 (408,2)는 logits이며 비교할 수 있는 값으로 변환을 위해선 두번째 축의 최대값이 있는 인덱스로 반환하여 그 인덱스를 사용!\r\n",
    "- label은 원래 데이터셋의 label 그대로임"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "source": [
    "import numpy as np\r\n",
    "preds = np.argmax(predictions.predictions,axis=-1)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 이제 preds와 label 비교가 가능\r\n",
    "- compute_metric 함수를 구현하기 위해 load_metric함수를 사용하여 MPRC 데이터세트와 관련 메트릭을 가져와보자"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "source": [
    "from datasets import load_metric\r\n",
    "\r\n",
    "metric = load_metric(\"glue\",\"mrpc\")\r\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'accuracy': 0.6764705882352942, 'f1': 0.7836065573770491}"
      ]
     },
     "metadata": {},
     "execution_count": 169
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- classifier가 랜덤으로 초기화 되므로 결과가 조금 달라질 수 있음\r\n",
    "- metric으로는 accuracy & f1\r\n",
    "\r\n",
    "이제 compute_metrics를 구해보자"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "source": [
    "def compute_metrics(eval_preds):\r\n",
    "    metric = load_metric(\"glue\",\"mrpc\")\r\n",
    "    logits, labels = eval_preds\r\n",
    "    predictions = np.argmax(logits, axis=-1)\r\n",
    "    return metric.compute(predictions=predictions,references=labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- compute_metrics 함수를 이제 Trainer의 argument로 넣자!\r\n",
    "- 단 traning_args도 다시 선언하며, 이 때 eval_stregy를 epoch로 설정\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "source": [
    "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\r\n",
    "model= AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=2)\r\n",
    "\r\n",
    "trainer=Trainer(\r\n",
    "    model,\r\n",
    "    training_args,\r\n",
    "    train_dataset = tokenized_datasets[\"train\"],\r\n",
    "    eval_dataset = tokenized_datasets[\"validation\"],\r\n",
    "    data_collator = data_collator,\r\n",
    "    tokenizer = tokenizer,\r\n",
    "    compute_metrics = compute_metrics\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\ChangYong/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\ChangYong/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainer.train()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# A full training\r\n",
    "- 이번엔 Trainer를 사용하지 않고 지난 섹션과 동일한 결과는 얻는 방법에 대해 공부해보자\r\n",
    "- 섹션 2의 데이터 처리를 완료했다는 가정 하에 진행하며, 다음은 필요한 모든 내용에 대한 코드이다!!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "source": [
    "from datasets import load_dataset\r\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\r\n",
    "\r\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\r\n",
    "checkpoint = \"bert-base-uncased\"\r\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\r\n",
    "\r\n",
    "def tokenize_function(example):\r\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\r\n",
    "\r\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\r\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loading cached processed dataset at C:\\Users\\ChangYong\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-2ff2f76fa35622cd.arrow\n",
      "Loading cached processed dataset at C:\\Users\\ChangYong\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-32e225bb550d8c33.arrow\n",
      "Loading cached processed dataset at C:\\Users\\ChangYong\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-36450eac73a2f315.arrow\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare for training\r\n",
    "- training을 위해선 몇가지 object를 정의해야 함\r\n",
    "- 첫번째는 dataloader, 다만 그 전에 토큰화된 데이터셋에 후처리를 조금 해야함!\r\n",
    "  - 모형이 사용하지 않는 값에 해당하는 열 제거\r\n",
    "  - 열 레이블 이름을 레이블로 변경(model에서 argument이름을 labels로 지정했기 때문)\r\n",
    "  - list 대신 Torch Tensor 형태로 반환하도록 설정\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\",\"sentence2\",\"idx\"])\r\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\",\"labels\")\r\n",
    "tokenized_datasets.set_format(\"torch\")\r\n",
    "\r\n",
    "tokenized_datasets[\"train\"].column_names"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['attention_mask', 'input_ids', 'labels', 'token_type_ids']"
      ]
     },
     "metadata": {},
     "execution_count": 187
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "이제 데이터 로더를 정의해보자"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "source": [
    "from torch.utils.data import DataLoader\r\n",
    "\r\n",
    "train_dataloader=DataLoader(\r\n",
    "    dataset = tokenized_datasets[\"train\"],\r\n",
    "    shuffle=True,\r\n",
    "    batch_size=8,\r\n",
    "    collate_fn=data_collator\r\n",
    "    )\r\n",
    "\r\n",
    "eval_dataloader = DataLoader(\r\n",
    "    tokenized_datasets[\"validation\"],\r\n",
    "    batch_size=8,\r\n",
    "    collate_fn=data_collator\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "잘 생성되는지 빠르게 확인 한 번 해보자"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "source": [
    "for batch in train_dataloader:\r\n",
    "    break\r\n",
    "{k: v.shape for k, v in batch.items()}"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'attention_mask': torch.Size([8, 65]),\n",
       " 'input_ids': torch.Size([8, 65]),\n",
       " 'labels': torch.Size([8]),\n",
       " 'token_type_ids': torch.Size([8, 65])}"
      ]
     },
     "metadata": {},
     "execution_count": 192
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "이제 모델을 불러오자"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "source": [
    "from transformers import AutoModelForSequenceClassification\r\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\ChangYong/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\ChangYong/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "잘 동작하는지 sample batch를 가지고 테스트해보자"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "source": [
    "outputs = model(**batch)\r\n",
    "print(outputs.loss, outputs.logits.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.8326, grad_fn=<NllLossBackward>) torch.Size([8, 2])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "이제 잘 생성됨을 확인했으니 거의 마지막 과정을 진행!!  \r\n",
    "optimizer와 learning rate scheduler가 필요하다!!  \r\n",
    "optimizer는 Adam과 유사한 AdamW와 비슷하지만, weight decay regularization이 포함됨"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "source": [
    "from transformers import AdamW\r\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Scheduler는 기본 lr=5e-5이고, 0으로 linear decay\r\n",
    "- 이것을 제대로 정의하려면, epoch과 batch의 개수를 고려하여 decay를 진행해야 한다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "source": [
    "from transformers import get_scheduler\r\n",
    "\r\n",
    "num_epochs = 3\r\n",
    "num_training_steps = num_epochs * len(train_dataloader)\r\n",
    "lr_scheduler = get_scheduler(\r\n",
    "    \"linear\",\r\n",
    "    optimizer=optimizer,\r\n",
    "    num_warmup_steps=0,\r\n",
    "    num_training_steps=num_training_steps\r\n",
    ")\r\n",
    "\r\n",
    "print(num_training_steps)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1377\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The training loop\r\n",
    "이제 거의 다 왔다..\r\n",
    "GPU를 사용할 경우 model을 gpu에 load한다!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "source": [
    "import torch\r\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\r\n",
    "model.to(device)\r\n",
    "device"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "metadata": {},
     "execution_count": 200
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "거의 다 됐고, 이제 tqdm 라이브러리를 활용해 교육 상황을 표시줄을 추가하자"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tqdm.auto import tqdm\r\n",
    "\r\n",
    "progress_bar = tqdm(range(num_training_steps))\r\n",
    "\r\n",
    "model.train()\r\n",
    "for epoch in range(num_epochs):\r\n",
    "    for batch in train_dataloader:\r\n",
    "        batch = {k: v.to(device) for k,v in batch.items()}\r\n",
    "        outputs = model(**batch)\r\n",
    "        loss = outputs.loss\r\n",
    "        loss.backward()\r\n",
    "\r\n",
    "        optimizer.step()\r\n",
    "        lr_scheduler.step()\r\n",
    "        optimizer.zero_grad()\r\n",
    "        progress_bar.update(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "이 과정은 초반과 유사하게, 어떠한 결과 지표도 설정하지 않아서 출력되지 않는다..   \r\n",
    "평가 loop을 추가해보자"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The evaluation loop\r\n",
    "- dataset library에서 제공하는 metric 사용\r\n",
    "- metric.compute는 이미 봤지만, metric에 add_batch를 활용하여 배치를 축적하고 모든 batch를 얻으면 compute하여 최종 결과를 얻는 방법으로 다음과 같이 구현"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "source": [
    "from datasets import load_metric\r\n",
    "\r\n",
    "metric = load_metric(\"glue\",\"mrpc\")\r\n",
    "model.eval()\r\n",
    "\r\n",
    "for batch in eval_dataloader:\r\n",
    "    batch = {k : v.to(device) for k,v in batch.items()}\r\n",
    "    with torch.no_grad():\r\n",
    "        outputs = model(**batch)\r\n",
    "    \r\n",
    "    logits = outputs.logits\r\n",
    "    predictions = torch.argmax(logits,dim=-1)\r\n",
    "    metric.add_batch(predictions=predictions,references=batch[\"labels\"])\r\n",
    "metric.compute()\r\n",
    "\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Supercharge your training loop with 🤗 Accelerate\r\n",
    "- 앞서 정의한 training loop은 단일 cpu or gpu에선 잘 작동\r\n",
    "- 하지만 accelerate library를 사용하면 multiple GPUs or TPUs를 활성화 가능\r\n",
    "- training & validation dataloader를 만드는 것부터 시작하면 다음과 같다"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from datasets import load_dataset\r\n",
    "from transformers import AdamW\r\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\r\n",
    "from transformers import DataCollatorWithPadding\r\n",
    "from transformers import get_scheduler\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "from tqdm.auto import tqdm\r\n",
    "\r\n",
    "#load dataset\r\n",
    "raw_datasets = load_dataset(\"glue\",\"mrpc\")\r\n",
    "\r\n",
    "#load pretrain object\r\n",
    "checkpoint = \"bert-base-uncased\"\r\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\r\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=2)\r\n",
    "\r\n",
    "#get tokenized_dataset\r\n",
    "def tokenzie_function(example):\r\n",
    "    return tokenizer(example[\"sentence1\"],example[\"sentence2\"],truncation=True)\r\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\r\n",
    "\r\n",
    "#get collator\r\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\r\n",
    "\r\n",
    "#post-process totenized_datasets\r\n",
    "tokenized_datasets = tokenized_datasets.rename_column([\"sentence1\",\"sentence2\",\"idx\"])\r\n",
    "tokenized_datasets.set_format(\"torch\")\r\n",
    "\r\n",
    "#dataloader\r\n",
    "train_dataloader = DataLoader(\r\n",
    "    dataset = tokenized_datasets[\"train\"],\r\n",
    "    shuffle=True,\r\n",
    "    batch_size=8,\r\n",
    "    collate_fn=data_collator\r\n",
    ")\r\n",
    "eval_dataloader = DataLoader(\r\n",
    "    dataset = tokenized_datasets[\"validation\"],\r\n",
    "    batch_size=8,\r\n",
    "    collate_fn=data_collator\r\n",
    ")\r\n",
    "\r\n",
    "#setting device\r\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\r\n",
    "model.to(device)\r\n",
    "\r\n",
    "# optmizer\r\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\r\n",
    "\r\n",
    "#sheduler\r\n",
    "num_epochs = 3\r\n",
    "num_training_steps = num_epochs * len(train_dataloader)\r\n",
    "lr_scheduler = get_scheduler(\r\n",
    "    \"linear\",\r\n",
    "    optimizer=optimizer,\r\n",
    "    num_warmup_steps = 0,\r\n",
    "    num_training_steps = num_training_steps\r\n",
    ")\r\n",
    "\r\n",
    "progress_bard = tqdm(range(num_training_steps))\r\n",
    "model.train()\r\n",
    "for epoch in range(num_epochs):\r\n",
    "    for batch in train_dataloader:\r\n",
    "        batch = {k: v.to(device) for k,v in batch.items()}\r\n",
    "        outputs = model(**batch)\r\n",
    "        loss = outputs.loss\r\n",
    "        loss.backward()\r\n",
    "\r\n",
    "        optimizer.step()\r\n",
    "        lr_scheduler.step()\r\n",
    "        optimizer.zero_grad()\r\n",
    "        progress_bar.update(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "전체 코드에서 일부 추가!! 및 제거"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# +\r\n",
    "from accelerate import Accelerator\r\n",
    "from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\r\n",
    "\r\n",
    "# + \r\n",
    "accelerator = Accelerator()\r\n",
    "\r\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\r\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\r\n",
    "\r\n",
    "# - \r\n",
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\r\n",
    "# - \r\n",
    "# model.to(device)\r\n",
    "\r\n",
    "# +\r\n",
    "train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\r\n",
    "     train_dataloader, eval_dataloader, model, optimizer\r\n",
    ")\r\n",
    "\r\n",
    "num_epochs = 3\r\n",
    "num_training_steps = num_epochs * len(train_dataloader)\r\n",
    "lr_scheduler = get_scheduler(\r\n",
    "    \"linear\",\r\n",
    "    optimizer=optimizer,\r\n",
    "    num_warmup_steps=0,\r\n",
    "    num_training_steps=num_training_steps\r\n",
    ")\r\n",
    "\r\n",
    "progress_bar = tqdm(range(num_training_steps))\r\n",
    "\r\n",
    "model.train()\r\n",
    "for epoch in range(num_epochs):\r\n",
    "    for batch in train_dataloader:\r\n",
    "        # -\r\n",
    "        # batch = {k: v.to(device) for k, v in batch.items()}\r\n",
    "        outputs = model(**batch)\r\n",
    "        loss = outputs.loss\r\n",
    "        # -\r\n",
    "        # loss.backward()\r\n",
    "        # +\r\n",
    "        accelerator.backward(loss)\r\n",
    "\r\n",
    "        optimizer.step()\r\n",
    "        lr_scheduler.step()\r\n",
    "        optimizer.zero_grad()\r\n",
    "        progress_bar.update(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 추가할 첫 번째 행은 library import 부분\r\n",
    "- 추가할 두 번째 줄은 환경을 살펴보고 적절한 분산 설정을 초기화하는 바로 연결 개체를 인스턴스화\r\n",
    "-  Accelerator()는 사용자 대신 장치 배치를 처리하므로 장치에 모델을 배치하는 라인을 제거\r\n",
    "  - 또는 device 대신 accelerator.device()를 사용해도 무방함\r\n",
    "- 그리고 삭제하는 부분들은 device나 batch 처리 관련 부분으로 만약 accelerator.device()를 사용하면 남겨둠\r\n",
    "- 하지만 loss.bacward()는 교체!!\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "- train.py에 수정한 전체 코드를 넣으면, 분산 설정에서 실행 할 수 있도록 가능\r\n",
    "- 분산 설정에서 테스트하려면 다음 명령을 싱행\r\n",
    "  - accelerate config\r\n",
    "    - 이걸 실행하면, 몇가지 answer에 답변하라는 prompt가 뜸\r\n",
    "  - accelerate launch train.py\r\n",
    "    - 이걸 실행하면 distributed training 시작\r\n",
    "- notebook에서 실행하려면 맨 아래 다음과 같은 코드 추가\r\n",
    "- ```python\r\n",
    "   from accelerate import notebook_launcher\r\n",
    "   notebook_launcher(training_function)\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "9d4b788d31f1e8a98acbf5edfe42af7a280714d85bd73a1cdc11a1af0199f3f5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}