{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What to expect?\r\n",
    "- Chapter 1~4\r\n",
    "  - transformers library\r\n",
    "  - 허깅페이스 허브를 다루는 방법\r\n",
    "  - 특정 dataset으로 fine-tune하는 방법\r\n",
    "  - 그 결과를 hub에 공유하는 방법\r\n",
    "- Chapter 5~8 (미구현)\r\n",
    "  - Dataset과 TOkenizer의 Basic\r\n",
    "  - 각 NLP Problem을 해결하는 방법\r\n",
    "- Chapter 9~12 (미구현)\r\n",
    "  - 메모리 효율이나, long seq에 대한 해결 방법\r\n",
    "  - 등등!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Latural Language Processing\r\n",
    "## What is NLP\r\n",
    "NLP는 인간의 언어와 관련된 모든 것을 이해하기 위한 머신 러닝과 언어학의 종합적인 학문으로, 개별 적인 단어 뿐만 아니라 문맥적 요소를 고려한 모든 단어를 이해하는 것이 목표다.\r\n",
    "\r\n",
    "NLP task는 아래와 같이 다양하게 존재한다.\r\n",
    "\r\n",
    "- **Classifying whole sentences**: Getting the sentiment of a review, detecting if an email is spam, determining if a sentence is grammatically correct or whether two sentences are logically related or not\r\n",
    "- **Classifying each word in a sentence**: Identifying the grammatical components of a sentence (noun, verb, adjective), or the named entities (person, location, organization)\r\n",
    "- **Generating text content**: Completing a prompt with auto-generated text, filling in the blanks in a text with masked words\r\n",
    "- **Extracting an answer from a text**: Given a question and a context, extracting the answer to the question based on the information provided in the context\r\n",
    "- **Generating a new sentence from an input text**: Translating a text into another language, summarizing a text\r\n",
    "\r\n",
    "다만 NLP는 텍스트에만 국한되지 않고, 음성 인식, 영상에서 오디오 샘플의 스크립트 생성, 이미지 설명과 같은 문제로 다룬다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transformer, What can they do?\r\n",
    "- Tranformer model + pipeline 활용법에 대해 공부\r\n",
    "\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Working with pipelines\r\n",
    "Transformers library에서 가장 기본적인 요소는 pipeline이다. pipline으로 다양한 처리 step과 model을 하나로 엮을 수 있다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "우선 학습을 위한 데이터셋을 설치하자"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "!pip install datasets transformers[sentencepiece]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\changyong\\miniconda3\\lib\\site-packages (1.12.1)\n",
      "Requirement already satisfied: transformers[sentencepiece] in c:\\users\\changyong\\miniconda3\\lib\\site-packages (4.10.3)\n",
      "Requirement already satisfied: dill in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from datasets) (3.7.4.post0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from datasets) (1.3.2)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from datasets) (2.25.1)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from datasets) (5.0.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: huggingface-hub<0.1.0,>=0.0.14 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from datasets) (0.0.17)\n",
      "Requirement already satisfied: packaging in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from datasets) (21.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from datasets) (2021.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from datasets) (1.20.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (3.10.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (3.0.12)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.6)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (5.1.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from pandas->datasets) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from transformers[sentencepiece]) (0.0.46)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from transformers[sentencepiece]) (2021.7.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from transformers[sentencepiece]) (5.4.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from transformers[sentencepiece]) (0.10.3)\n",
      "Requirement already satisfied: sentencepiece==0.1.91 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from transformers[sentencepiece]) (0.1.91)\n",
      "Requirement already satisfied: protobuf in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from transformers[sentencepiece]) (3.17.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from sacremoses->transformers[sentencepiece]) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from sacremoses->transformers[sentencepiece]) (8.0.1)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## sentiment analysis\r\n",
    "파이프라인을 불러와서, 예제와 같이 감성분류를 실행해보자"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from transformers import pipeline\r\n",
    "classifier = pipeline(\"sentiment-analysis\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598047137260437}]"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "위와 같이 해당 문장은 긍정이 95%인 확률로 분류했음을 확인할 \r\n",
    "수 있다.   \r\n",
    "이번엔 두 문장을 한번에 분석해보자"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "classifier([\r\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\", \r\n",
    "    \"I hate this so much!\"\r\n",
    "])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598047137260437},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "두 문장이 다음과 같이 분석되었다. 그렇다면 이건 어떻게 진행되는 것일까?\r\n",
    "pipeline()함수를 사용하여 특정 task에 대한 pipeline 요소를 불러오게 되면, 해당 pipeline에 필요한 model이 다운된다. 이 과정에선 **distilbert-base-uncased-finetuned-sst-2-english**이 다운 된것을 확인할 수 있다.\r\n",
    "\r\n",
    "파이프라인은 세 단계로 구성된다.\r\n",
    "1. 우선 텍스트 전처리를 통해 모델의 input 형태로 변환해주고\r\n",
    "2. 변환된 inputs을 model에 넣는다.\r\n",
    "3. 예측 결과를 반환한다.   \r\n",
    "\r\n",
    "파이프라인은 아래와 같이 task에 사용이 가능하다.\r\n",
    "- feature-extraction\r\n",
    "- fill-text\r\n",
    "- NER\r\n",
    "- QA\r\n",
    "- sentiment-analysis\r\n",
    "- summarization\r\n",
    "- text-generation\r\n",
    "- translation\r\n",
    "- zero-shot-classification\r\n",
    "- etc..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Zero-shot classification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "보통 라벨이 부착되지 않는 text를 분류하는 것으로 프로젝트를 시작으로, 텍스트에 주석을 다는 것은 많은 시간과 도메인 전문 지식이 필요하다. 따라서 zero-shot classification을 하면 분류에 사용할 레이블을 지정할 수 있다.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "우선 제로샷 분류를 위한 파이프라인을 불러오자"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "from transformers import pipeline\r\n",
    "classifier = pipeline(\"zero-shot-classification\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli (https://huggingface.co/facebook/bart-large-mnli)\n",
      "Downloading: 100%|██████████| 1.15k/1.15k [00:00<00:00, 578kB/s]\n",
      "Downloading: 100%|██████████| 1.63G/1.63G [03:59<00:00, 6.80MB/s]\n",
      "Downloading: 100%|██████████| 26.0/26.0 [00:00<00:00, 13.2kB/s]\n",
      "Downloading: 100%|██████████| 899k/899k [00:01<00:00, 716kB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:01<00:00, 358kB/s]\n",
      "Downloading: 100%|██████████| 1.36M/1.36M [00:02<00:00, 518kB/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "위와 같이 bert-large-mnli pretrained model이 다운로드되는 것을 확인할 수 있다.\r\n",
    "이제 label을 지정하고 classification을 해보자"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "classifier(\"This is a course about the Transformers library\",\r\n",
    "          candidate_labels = [\"education\",\"politics\",\"business\"])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.8445971608161926, 0.11197549849748611, 0.04342736303806305]}"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "84.5% 확률로 education으로 분류가 되었다.   \r\n",
    "이 파이프 라인은 제로샷으로, 분석할 데이터를 위한 추가적은 finetune을 하지 않고 동작하기 때문이다. 제로샷은 pre-trained 기반으로 내가 분류하고자 한 labels에 대해 확률로 결과를 반환해준다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text generation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Text generation task는 text 일부를 가지고 모델이 다음 text를 생성하는 task이다.   \r\n",
    "이 task는 randomness가 포함되어, 예제와 다른 결과를 출력하기도 한다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "from transformers import pipeline\r\n",
    "generator = pipeline(\"text-generation\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No model was supplied, defaulted to gpt2 (https://huggingface.co/gpt2)\n",
      "Downloading: 100%|██████████| 665/665 [00:00<00:00, 666kB/s]\n",
      "Downloading: 100%|██████████| 548M/548M [01:09<00:00, 7.88MB/s]\n",
      "Downloading: 100%|██████████| 1.04M/1.04M [00:01<00:00, 785kB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 465kB/s]\n",
      "Downloading: 100%|██████████| 1.36M/1.36M [00:01<00:00, 908kB/s] \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "text-generation의 default model은 gpt2임을 확인할 수 있다. 이제 텍스트 생성을 해보자"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "generator(\"In this course, we will teach you how to\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to use a real-time JavaScript parser to perform a data conversion from one data point onto another. In order to perform this conversion, create a JSON document and add the following:\\n\\nmyDataObject'}]"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "이번엔, gpt-2가 아닌 distillgpt2 모델을 불러오고, maxlen과 returen seq 개수를 지정하여 생성해보자"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "from transformers import pipeline\r\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 762/762 [00:00<00:00, 389kB/s]\n",
      "Downloading: 100%|██████████| 353M/353M [00:54<00:00, 6.48MB/s]\n",
      "Downloading: 100%|██████████| 1.04M/1.04M [00:02<00:00, 502kB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:01<00:00, 360kB/s]\n",
      "Downloading: 100%|██████████| 1.36M/1.36M [00:03<00:00, 380kB/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "generator(\"In this course, we will teach you how to\",\r\n",
    "          max_length = 15,\r\n",
    "          num_return_sequences = 2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to create a new class with'},\n",
       " {'generated_text': 'In this course, we will teach you how to handle this issue. If'}]"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mask filling\r\n",
    "다음 task는 mask filling으로 동일하게 진행해보자   \r\n",
    "단, mask filling은 특정 단어를 <mask>로 변환하여 input을 넣어줘야 한다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "from transformers import pipeline\r\n",
    "unmasker = pipeline(\"fill-mask\")\r\n",
    "unmasker(\"This course will teach you all about <mask> models.\",\r\n",
    "         top_k=2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No model was supplied, defaulted to distilroberta-base (https://huggingface.co/distilroberta-base)\n",
      "Downloading: 100%|██████████| 480/480 [00:00<00:00, 466kB/s]\n",
      "Downloading: 100%|██████████| 331M/331M [00:50<00:00, 6.61MB/s]\n",
      "Downloading: 100%|██████████| 899k/899k [00:02<00:00, 403kB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:01<00:00, 456kB/s]\n",
      "Downloading: 100%|██████████| 1.36M/1.36M [00:03<00:00, 446kB/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'sequence': 'This course will teach you all about mathematical models.',\n",
       "  'score': 0.1961982101202011,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical'},\n",
       " {'sequence': 'This course will teach you all about computational models.',\n",
       "  'score': 0.04052715376019478,\n",
       "  'token': 38163,\n",
       "  'token_str': ' computational'}]"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "해당 task에는 distilroberta-base가 기본 모델이며,   \r\n",
    "top_k = n에서, n 개수만큼 <mask>를 대체할 단어를 출력한다.   \r\n",
    "이 때, 각 단어의 확률값과, 해당 단어에 해당하는 token_id와 token_str가 주어진다"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Named entity recognition\r\n",
    "NER은 특정 token이 location인지, person인지 등등 어떤 개체명으로 분류하는지에 대한 task이다.   \r\n",
    "여기서 grouped_entities=True는 subword로 된 것중 특정 entity를 group하는 것에 대한 옵션이다.   \r\n",
    "예를 들어, hugging, face가 있을 때 하나의 entity임을 나타내기 위해 True를 주면 \"hugging face\"를 하나의 entity로 인식한다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "from transformers import pipeline\r\n",
    "ner = pipeline(\"ner\", grouped_entities=True)\r\n",
    "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english)\n",
      "Downloading: 100%|██████████| 998/998 [00:00<00:00, 506kB/s]\n",
      "Downloading: 100%|██████████| 1.33G/1.33G [03:04<00:00, 7.25MB/s]\n",
      "Downloading: 100%|██████████| 60.0/60.0 [00:00<00:00, 57.4kB/s]\n",
      "Downloading: 100%|██████████| 213k/213k [00:00<00:00, 215kB/s]\n",
      "C:\\Users\\ChangYong\\miniconda3\\lib\\site-packages\\transformers\\pipelines\\token_classification.py:154: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9981694,\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.97960204,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.99321055,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "from transformers import pipeline\r\n",
    "\r\n",
    "ner = pipeline(\"ner\", grouped_entities=False)\r\n",
    "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english)\n",
      "C:\\Users\\ChangYong\\miniconda3\\lib\\site-packages\\transformers\\pipelines\\token_classification.py:154: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.NONE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER',\n",
       "  'score': 0.9993828,\n",
       "  'index': 4,\n",
       "  'word': 'S',\n",
       "  'start': 11,\n",
       "  'end': 12},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99815476,\n",
       "  'index': 5,\n",
       "  'word': '##yl',\n",
       "  'start': 12,\n",
       "  'end': 14},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99590725,\n",
       "  'index': 6,\n",
       "  'word': '##va',\n",
       "  'start': 14,\n",
       "  'end': 16},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9992327,\n",
       "  'index': 7,\n",
       "  'word': '##in',\n",
       "  'start': 16,\n",
       "  'end': 18},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.97389334,\n",
       "  'index': 12,\n",
       "  'word': 'Hu',\n",
       "  'start': 33,\n",
       "  'end': 35},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.976115,\n",
       "  'index': 13,\n",
       "  'word': '##gging',\n",
       "  'start': 35,\n",
       "  'end': 40},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.98879766,\n",
       "  'index': 14,\n",
       "  'word': 'Face',\n",
       "  'start': 41,\n",
       "  'end': 45},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.99321055,\n",
       "  'index': 16,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question answering"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "from transformers import pipeline\r\n",
    "\r\n",
    "question_answerer = pipeline(\"question-answering\")\r\n",
    "question_answerer(\r\n",
    "    question=\"Where do I work?\",\r\n",
    "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\"\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad (https://huggingface.co/distilbert-base-cased-distilled-squad)\n",
      "Downloading: 100%|██████████| 473/473 [00:00<00:00, 242kB/s]\n",
      "Downloading: 100%|██████████| 261M/261M [00:32<00:00, 8.08MB/s]\n",
      "Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 14.5kB/s]\n",
      "Downloading: 100%|██████████| 213k/213k [00:00<00:00, 283kB/s]\n",
      "Downloading: 100%|██████████| 436k/436k [00:01<00:00, 425kB/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'score': 0.6949764490127563, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summarization\r\n",
    "- 요약 task로 max_length or a min_length를 지정할 수 있다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "from transformers import pipeline\r\n",
    "\r\n",
    "summarizer = pipeline(\"summarization\")\r\n",
    "summarizer(\"\"\"\r\n",
    "    America has changed dramatically during recent years. Not only has the number of \r\n",
    "    graduates in traditional engineering disciplines such as mechanical, civil, \r\n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of \r\n",
    "    the premier American universities engineering curricula now concentrate on \r\n",
    "    and encourage largely the study of engineering science. As a result, there \r\n",
    "    are declining offerings in engineering subjects dealing with infrastructure, \r\n",
    "    the environment, and related issues, and greater concentration on high \r\n",
    "    technology subjects, largely supporting increasingly complex scientific \r\n",
    "    developments. While the latter is important, it should not be at the expense \r\n",
    "    of more traditional engineering.\r\n",
    "\r\n",
    "    Rapidly developing economies such as China and India, as well as other \r\n",
    "    industrial countries in Europe and Asia, continue to encourage and advance \r\n",
    "    the teaching of engineering. Both China and India, respectively, graduate \r\n",
    "    six and eight times as many traditional engineers as does the United States. \r\n",
    "    Other industrial countries at minimum maintain their output, while America \r\n",
    "    suffers an increasingly serious decline in the number of engineering graduates \r\n",
    "    and a lack of well-educated engineers.\r\n",
    "\"\"\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n",
      "Downloading: 100%|██████████| 1.80k/1.80k [00:00<00:00, 899kB/s]\n",
      "Downloading: 100%|██████████| 1.22G/1.22G [02:42<00:00, 7.53MB/s]\n",
      "Downloading: 100%|██████████| 26.0/26.0 [00:00<00:00, 12.8kB/s]\n",
      "Downloading: 100%|██████████| 899k/899k [00:01<00:00, 716kB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:01<00:00, 449kB/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'summary_text': ' America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil,    electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .'}]"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Translation\r\n",
    "번역 task로 max_length or a min_length를 지정할 수 있다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "from transformers import pipeline\r\n",
    "\r\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\r\n",
    "translator(\"Ce cours est produit par Hugging Face.\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 1.29k/1.29k [00:00<00:00, 642kB/s]\n",
      "Downloading: 100%|██████████| 301M/301M [00:43<00:00, 6.94MB/s]\n",
      "Downloading: 100%|██████████| 42.0/42.0 [00:00<00:00, 40.1kB/s]\n",
      "Downloading: 100%|██████████| 802k/802k [00:01<00:00, 660kB/s]\n",
      "Downloading: 100%|██████████| 778k/778k [00:07<00:00, 98.6kB/s]\n",
      "Downloading: 100%|██████████| 1.34M/1.34M [00:05<00:00, 264kB/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'translation_text': 'This course is produced by Hugging Face.'}]"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# How do Transformers work?\r\n",
    "- 이번 section에선 transformer architecture에 대해 공부하는 과정이다.\r\n",
    "- https://huggingface.co/course/chapter1/4?fw=pt"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Encoder models\r\n",
    "- https://huggingface.co/course/chapter1/5?fw=pt\r\n",
    "- 문장의 이해를 바탕으로 하는 task에 특화 - sentence classification, named entity recognition (and more generally word classification), and extractive question answering.\r\n",
    "- albert, bert, distillbert, electra, roberta"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decoder models\r\n",
    "- https://huggingface.co/course/chapter1/6?fw=pt\r\n",
    "- text Generation에 특화\r\n",
    "- [CTRL](https://huggingface.co/transformers/model_doc/ctrl.html), GPT, GPT-2, Transformer XL"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Encoder-Decoder Models(= seq2seq models)\r\n",
    "- https://huggingface.co/course/chapter1/7?fw=pt\r\n",
    "- use both of the transformer architecture\r\n",
    "- generating new sentences에 특화(요약,번역,QA)\r\n",
    "- BART, mBART, Marian, T5"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bias and limitations\r\n",
    "- https://huggingface.co/course/chapter1/8?fw=pt\r\n",
    "- production에 pretrained or fine-tuned model을 사용할 매우 유용하게 사용가능하지만 어느 정도 제한사항이 존재\r\n",
    "- 많은 양의 데이터를 스크래핑할 경우 좋은 데이터만 수집할 수는 없음\r\n",
    "- 데이터는 기본적으로 인간이 가지고 있는 내재적 편견(인종차별, 성차별, 나이)등이 포함되어 있기에, fine-tune을 한다고 하더라도 이러한 편향은 사라지지 않음"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "!pip install datasets transformers[sentencepiece]\r\n",
    "\r\n",
    "from transformers import pipeline\r\n",
    "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\changyong\\miniconda3\\lib\\site-packages (1.12.1)\n",
      "Requirement already satisfied: transformers[sentencepiece] in c:\\users\\changyong\\miniconda3\\lib\\site-packages (4.10.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from datasets) (1.20.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from datasets) (2.25.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from datasets) (1.3.2)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from datasets) (21.0)\n",
      "Requirement already satisfied: huggingface-hub<0.1.0,>=0.0.14 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from datasets) (0.0.17)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from datasets) (5.0.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from datasets) (2021.9.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from datasets) (3.7.4.post0)\n",
      "Requirement already satisfied: dill in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (5.1.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from pandas->datasets) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from transformers[sentencepiece]) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from transformers[sentencepiece]) (2021.7.6)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from transformers[sentencepiece]) (0.0.46)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from transformers[sentencepiece]) (0.10.3)\n",
      "Requirement already satisfied: protobuf in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from transformers[sentencepiece]) (3.17.3)\n",
      "Requirement already satisfied: sentencepiece==0.1.91 in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from transformers[sentencepiece]) (0.1.91)\n",
      "Requirement already satisfied: click in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from sacremoses->transformers[sentencepiece]) (8.0.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\changyong\\miniconda3\\lib\\site-packages (from sacremoses->transformers[sentencepiece]) (1.0.1)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 570/570 [00:00<00:00, 537kB/s]\n",
      "Downloading: 100%|██████████| 440M/440M [01:18<00:00, 5.60MB/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 26.1kB/s]\n",
      "Downloading: 100%|██████████| 232k/232k [00:01<00:00, 129kB/s]\n",
      "Downloading: 100%|██████████| 466k/466k [00:01<00:00, 345kB/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "result = unmasker(\"This man works as a [MASK].\")\r\n",
    "print([r[\"token_str\"] for r in result])\r\n",
    "\r\n",
    "result = unmasker(\"This woman works as a [MASK].\")\r\n",
    "print([r[\"token_str\"] for r in result])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']\n",
      "['nurse', 'maid', 'teacher', 'waitress', 'prostitute']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "다음과 같이 남성과 여성의 work에 대해 masking을 하고 이것을 예측해보면, 편향에 의해 직업에 차이가 발생함을 알 수 있다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## summary\r\n",
    "\r\n",
    "|Model | Examples |Tasks|\r\n",
    "|------|----------|-----|\r\n",
    "|Encoder|ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa|Sentence classification, named entity recognition, extractive question answering|\r\n",
    "|Decoder|CTRL, GPT, GPT-2, Transformer XL|Text generation\r\n",
    "|Encoder-decoder|BART, T5, Marian, mBART|Summarization, translation, generative question answering|"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "9d4b788d31f1e8a98acbf5edfe42af7a280714d85bd73a1cdc11a1af0199f3f5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}