{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Incroduction\r\n",
    "- Chp1에선 hihg-level의 pipline API를 사용하여 여러 task에 대한 transformer model을 사용해봤다.\r\n",
    "- pipeline api가 편하더라도, 이것에 대한 이해를 바탕으로 해야 더 유연하게 문제를 해결할 수 있다.\r\n",
    "- 이번 챕터에선 이 부분에 대해서 알아보고자 한다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Behind the pipeline\r\n",
    "Chp 1을 상기해보면, 특정 task에 해당하는 pipeline을 불러오고, 여기에 inputs을 넣어서 결과를 출력했다.\r\n",
    "\r\n",
    "이 과정을 간략히 본다면 다음과 같다.\r\n",
    "- Raw Text input => Tokenizer -> input ids => Model -> logits=> Post-processing -> predictions\r\n",
    "아래와 같이 결과가 나오고, Chp2에서는 이 과정들을 살펴보자"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from transformers import pipeline\r\n",
    "classifier = pipeline(\"sentiment-analysis\")\r\n",
    "classifier([\r\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\", \r\n",
    "    \"I hate this so much!\",\r\n",
    "])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598047137260437},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing with a tokenizer\r\n",
    "- Pipeline의 첫 step은 raw input을 token으로 변경 후 정수로 변환해주는 단계이며, 이 과정에서 tokenizer를 사용\r\n",
    "- AutoTokenizer 클래스의 from_pretrained를 사용\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from transformers import AutoTokenizer\r\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\r\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- raw input이 tokenizer를 통과하면 모델에 넣을 수 있는 형태의 input으로 변경됨\r\n",
    "- 이제 입력 input IDs를 Tensor 형태로 변환이 필요"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "raw_inputs = [\r\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\", \r\n",
    "    \"I hate this so much!\",\r\n",
    "]\r\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True,return_tensors=\"pt\")\r\n",
    "inputs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
       "          2607,  2026,  2878,  2166,  1012,   102],\n",
       "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "padding과 truncation은 이후에 설명하고,   \r\n",
    "우선 Pytorch tensor로 변환하기 위해 return_tensors=\"pt\"로 지정하면 위와 같이 출력된다.   \r\n",
    "input_ids는 각 seq에 대해 토큰화 후 정수인코딩을 한 결과이다.   \r\n",
    "attention_mask는 이후 챕터에서 설명!!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Going through the model\r\n",
    "사용할 모델은 tokenizer와 동일한 방법으로 호출이 가능하다"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from transformers import AutoModel\r\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\r\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "이 모델 구조는  base Transformer module만 포함되어 있어, inputs이 주어지면 hidden states의 outputs이 출력된다.   \r\n",
    "hidden states 자체도 유용하지만, 일반적으로 이 outputs은 다른 part의 inputs으로 사용된다.   \r\n",
    "Chp1을 상기해보면, 여러 task가 있지만, 동일한 architecture의 model을 사용하고 마지막 head만 다르다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A high-dimensional vector?\r\n",
    "transformer의 output vector는 다음과 같은 차원을 갖는다.\r\n",
    "- batch size : sequence 개수\r\n",
    "- sequence length : sequence 길이\r\n",
    "- hidden size : vector dimension\r\n",
    "\r\n",
    "여기서 high-dimensional vector라고 불리는 이유는, vector의 마지막 rank의 차원이 매우 크기 때문이다. (128,256,512,768,3072,or more...)   \r\n",
    "\r\n",
    "model에 input을 넣는 방법은 다음과 같다"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "print(\"inputs : \", inputs)\r\n",
    "outputs = model(**inputs)\r\n",
    "print(\"*\"*100)\r\n",
    "print(outputs.keys())\r\n",
    "print(\"outputs_size : \", outputs.last_hidden_state.shape)\r\n",
    "print(\"outputs : \", outputs.last_hidden_state)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "inputs :  {'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "****************************************************************************************************\n",
      "odict_keys(['last_hidden_state'])\n",
      "outputs_size :  torch.Size([2, 16, 768])\n",
      "outputs :  tensor([[[-0.1798,  0.2333,  0.6321,  ..., -0.3017,  0.5008,  0.1481],\n",
      "         [ 0.2758,  0.6497,  0.3200,  ..., -0.0760,  0.5136,  0.1329],\n",
      "         [ 0.9046,  0.0985,  0.2950,  ...,  0.3352, -0.1407, -0.6464],\n",
      "         ...,\n",
      "         [ 0.1466,  0.5661,  0.3235,  ..., -0.3376,  0.5100, -0.0561],\n",
      "         [ 0.7500,  0.0487,  0.1738,  ...,  0.4684,  0.0030, -0.6084],\n",
      "         [ 0.0519,  0.3729,  0.5223,  ...,  0.3584,  0.6500, -0.3883]],\n",
      "\n",
      "        [[-0.2937,  0.7283, -0.1497,  ..., -0.1187, -1.0227, -0.0422],\n",
      "         [-0.2206,  0.9384, -0.0951,  ..., -0.3643, -0.6605,  0.2407],\n",
      "         [-0.1536,  0.8987, -0.0728,  ..., -0.2189, -0.8528,  0.0710],\n",
      "         ...,\n",
      "         [-0.3017,  0.9002, -0.0200,  ..., -0.1082, -0.8412, -0.0861],\n",
      "         [-0.3338,  0.9674, -0.0729,  ..., -0.1952, -0.8181, -0.0634],\n",
      "         [-0.3454,  0.8824, -0.0426,  ..., -0.0993, -0.8329, -0.1065]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model heads: Making sense out of numbers\r\n",
    "\r\n",
    "model head는 hidden state vectors를 input으로 하여 다른 차원으로 투영하고 결과를 출력하며, 다음과 같이 진행된다.\r\n",
    "![](https://huggingface.co/course/static/chapter2/transformer_and_head.png)\r\n",
    "\r\n",
    "outputs이 특정 task를 위한 head의 input으로 사용된다.   \r\n",
    "transformers에는 다양한 architectures가 있으며, 각 architecture는 특정 작업에 focus되어 설계되었다.   \r\n",
    "다음은 transformers에 있는 architecture를 불러오기 위한 일부 방법이다.\r\n",
    "- *Model (retrieve the hidden states)\r\n",
    "- *ForCausalLM\r\n",
    "- *ForMaskedLM\r\n",
    "- *ForMultipleChoice\r\n",
    "- *ForQuestionAnswering\r\n",
    "- *ForSequenceClassification\r\n",
    "- *ForTokenClassification\r\n",
    "- and others 🤗\r\n",
    "\r\n",
    "한 예로, sequence classification head를 위한 model architecture를 불러오려면, automodel이 아니라 ForSequenceClassification이다.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "from transformers import AutoModelForSequenceClassification\r\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\r\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "outputs = model(**inputs)\r\n",
    "print(\"inputs: \", inputs)\r\n",
    "print(\"outputs: \", outputs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "inputs:  {'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "outputs:  SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "해당 결과는 logits으로 softmax를 통과하여 감성분류 결과를 확인해야 한다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "import torch\r\n",
    "pred = torch.nn.functional.softmax(outputs.logits,dim=-1)\r\n",
    "pred"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[4.0195e-02, 9.5980e-01],\n",
       "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "이제 해당 값을 기준으로 label을 지정해야 하며 이때, model의 method중 id2label를 보면 각 label을 확인 가능하다"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "model.config.id2label"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Models\r\n",
    "- model을 만들고 사용하는 방법에 대해 알아보기\r\n",
    "- 일반적으로 checkpoint로부터 모델을 인스턴스화 할 때는 automodel을 사용\r\n",
    "- AutoModel은 입력한 checkpoint를 기반으로 적한한 모델 아키텍처를 자동으로 추축한 다음 불러오므로 매우 사용하기 편리함\r\n",
    "- 단, 사용할 모델 유형을 알고 있는 경우 직접 사용 가능\r\n",
    "- bert model로 이 기능이 어떻게 동작하는지 확인해보자"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating a Transformer\r\n",
    "### Different loading methods\r\n",
    "우선 BERT model을 만들기에 앞서 우선 configuration을 load해야 한다.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "from transformers import BertConfig, BertModel\r\n",
    "\r\n",
    "# Buliding the config\r\n",
    "config = BertConfig()\r\n",
    "print(\"config for BERT\", \"\\n\",config)\r\n",
    "\r\n",
    "# Building the model from the config\r\n",
    "model = BertModel(config)\r\n",
    "print(\"*\"*50)\r\n",
    "print(model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "config for BERT \n",
      " BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "**************************************************\n",
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "model config를 불러왔지만, weight와 bias가 랜덤초기화가 된 상태다. 따라서 해당 상태로 그냥 사용할 순 없고 학습을 시켜야 한다. 다만 학습에는 많은 cost와 data가 필요하므로, 불필요한 중복학습을 방지하기 위해선 이미 학습되어 공유된 모델을 재사용하면 된다.\r\n",
    "\r\n",
    "이 때 from_pretrained method를 활용하면 된다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "from transformers import BertModel\r\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "앞서 실습한 것과 같이, BertModel로 AutoModel 대체 가능하다.  \r\n",
    "\r\n",
    "지금부터 진행할 실습에서는 checkpoint-agnostic한 코드를 사용 할 예정이므로, 한 체크포인트에서 코드가 잘 동작하면 다른 체크포인트에서도 완벽히 동작할 것이다.   \r\n",
    "\r\n",
    "이 코드에서는 BertConfig를 사용하지 않고, Bert-base-case 식별자를 통해 모델을 로드했다. 이는 Bert 작성자가 직접 교육한 모델의 체크포인트로, 모델에 대한 자세한 것은 modelcard를 참고하면 된다. \r\n",
    "\r\n",
    "불러온 모델은 체크포인트의 모든 가중치로 초기화되며, downstream task를 위해 finetune만 수행하면 되므로, 빠르게 학습하여 좋은 결과를 얻을 수 있다.\r\n",
    "\r\n",
    "가중치가 캐시 폴더에 다운로드 및 캐시화되어 향후 다시 다운할 필요는 없고, 기본적으로 ~/chche/huggingface/transformers 위치에서 진행된다.\r\n",
    "\r\n",
    "HF_HOME 환경변수를 설정하여 customizing이 가능하다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Saving methods\r\n",
    "모델 save는 save_pretrained moethod를 사용하면 된다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "model.save_pretrained(\"directory_on_my_computer\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using a Transformer model for inference\r\n",
    "model은 tokenizer에서  생성한 inputs에 대해서만 처리가 가능하다. 토크나이저를 다루기 이전에, 모델이 어떤 inputs을 받는지 알아보자"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "sequences = [\r\n",
    "  \"Hello!\",\r\n",
    "  \"Cool.\",\r\n",
    "  \"Nice!\"\r\n",
    "]\r\n",
    "\r\n",
    "encoded_sequences = [\r\n",
    "  [ 101, 7592,  999,  102],\r\n",
    "  [ 101, 4658, 1012,  102],\r\n",
    "  [ 101, 3835,  999,  102]\r\n",
    "]\r\n",
    "encoded_sequences"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[101, 7592, 999, 102], [101, 4658, 1012, 102], [101, 3835, 999, 102]]"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "tokenizers는 input을 vocab의 idex로 mapping을 하는데, 상기 과정에서 sequences가 encoded_sequenced로 정수인코딩이 되었다.\r\n",
    "\r\n",
    "torch의 inputs은 tensor이므로 변환을 해준다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "import torch\r\n",
    "model_inputs = torch.tensor(encoded_sequences)\r\n",
    "model_inputs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 101, 7592,  999,  102],\n",
       "        [ 101, 4658, 1012,  102],\n",
       "        [ 101, 3835,  999,  102]])"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using the tensors as inputs to the model\r\n",
    "이렇게 만든 tensor를 model의 input으로 넣어준다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "output = model(model_inputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenizers\r\n",
    "- NLP 파이프라인의 핵심 구성 요소 중 하나\r\n",
    "- 텍스트를 모델이 처리가능한 형태로 변환(str-> int)\r\n",
    "- 몇 가지 토큰화 알고리즘을 알아보고, 몇 가지 질문에 대해 대답을 해보자"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word-based\r\n",
    "- spacebar 혹은 spacebar + punctuation 기준으로 나눌 수 있음\r\n",
    "- 매우 쉬운 방법으로 종종 적절한 결과를 산출\r\n",
    "- 단, 모든 단어에 대한 vocabulary를 만드려면 매우 큰 courpus가 필요\r\n",
    "  - computer는 run과 runs, running, ran 등을 다 다른 단어로 판단\r\n",
    "- 하지만, 그럼에도 없는 단어는 <UNK>으로 사용하는 unknown token을 사용하여 mapping시킨다.\r\n",
    "- 또 다른 방법으로는 더 작은 단위로 token을 쪼개는 것이다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Character-based\r\n",
    "- character level로 토큰화를 진행하면 다음과 같은 장단점이 있음\r\n",
    "  - pros : small vocabulary, unk token을 최소화 가능\r\n",
    "  - cons : \r\n",
    "    - 언어에 따라 하나의 character가 담고있는 의미의 크기가 다를 수 있음\r\n",
    "    - 매우 긴 sequence를 token화하면 너무 커짐\r\n",
    "  - 따라서 character level과 word level의 중간인 subword level이 있음"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Subward tokenization\r\n",
    "- principle : 자주 사용되는 단어는 분해 X, 희귀한 단어는 하위 단어로 분해\r\n",
    "  - e.g.) annoyingly => annoying, ly\r\n",
    "  - annoying과 ly는 기존보다 이 상태에서 더 자주 등장하므로 분해\r\n",
    "- 해당 방식을 활용하면 unk token 사용을 최소화 가능\r\n",
    "- subword에는 다양한 알고리즘이 존재\r\n",
    "  - Byte-level BPE, as used in GPT-2\r\n",
    "    - 빈도수 기반\r\n",
    "  - WordPiece, as used in BERT\r\n",
    "    - BPE와 같지만 likelihood 기반\r\n",
    "  - SentencePiece or Unigram, as used in several multilingual models\r\n",
    "    - BPE + unigram-LM 기반\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading and saving\r\n",
    "- tokenizer의 save & load는 model과 동일\r\n",
    "  - .from_pretrained(체크포인트), save_pretrained(체크포인트)\r\n",
    "- 모델이 가중치를 저장하는 것과 같이, tokenizer는 vocab을 저장\r\n",
    "- tokenizer도 AutoTokenizer도 있으며, BertTokenizer를 가져오는 것과 동일하게 가져올 수 있음"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "from transformers import AutoTokenizer, BertTokenizer\r\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\r\n",
    "tokenizer_ = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "print(\"AutoTokenizer: \",tokenizer)\r\n",
    "print(\"*\"*50)\r\n",
    "print(\"BertTokenizer: \",tokenizer_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AutoTokenizer:  PreTrainedTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "**************************************************\n",
      "BertTokenizer:  PreTrainedTokenizer(name_or_path='bert-base-cased', vocab_size=28996, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "for k,v in tokenizer(\"Using a Transformer network is simple\").items():\r\n",
    "    print(f\"{k} : {v}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input_ids : [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102]\n",
      "token_type_ids : [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "attention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# save tokenizer \r\n",
    "tokenizer.save_pretrained(\"directory_on_my_computer\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- token_type_ids는 chp3에서 다룰 예정   \r\n",
    "\r\n",
    "일단은 attention mask와 inputs에 대해 공부해보자"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Encoding\r\n",
    "- 인코딩 : 텍스트를 숫자로 변환하는 과정\r\n",
    "- two-step : 1) tokenization 2) 정수로 변환\r\n",
    "- tokenizer마다 저마다의 규칙이 있으므로, 모델의 이름을 사용하여 tokenizer를 저장하고, 불러올 땐 tokenizer도 함께 불러와서 사용해야 함.\r\n",
    "- vocabulary 역시도 동일한 것을 사용해야 하며, 이 두 가지를 해주는 것이 from_pretrained() 함수"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenization : seq를 token화 하는 과정"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "from transformers import AutoTokenizer\r\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\r\n",
    "sequence = \"Using a Transformer network is simple\"\r\n",
    "tokens = tokenizer.tokenize(sequence)\r\n",
    "print(\"bert tokenizer 사용 결과\",\"\\n\",tokens)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "bert tokenizer 사용 결과 \n",
      " ['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### From tokens to input IDs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\r\n",
    "print(ids)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[7993, 170, 13809, 23763, 2443, 1110, 3014]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decoding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\r\n",
    "print(decoded_string)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using a transformer network is simple\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "decode는 generate task에서 매우 우용하게 사용!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Handling multiple sequences\r\n",
    "- 기존 section에서 간단한 예제를 살펴봤지만 다음과 같이 의문이 생길 수 있음\r\n",
    "  - 여러 문장을 다루는 방법\r\n",
    "  - 길이가 다른 문장들을 다루는 방법\r\n",
    "  - 정수인코딩만이 모델을 잘 작동하기 위한 방법인지\r\n",
    "  - 매우 긴 문장에 대해 다루는 방법\r\n",
    "- 이번 section에서 해당 의문들에 대해 속시원하게 풀고 갈 예정"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Models expect a batch of inputs\r\n",
    "우선 기존 section에서와 같이, sequence를 숫자로 mapping한 다음 tensor로 변환하는 과정을 진행"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "import torch\r\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\r\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\r\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\r\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "source": [
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\r\n",
    "tokens = tokenizer.tokenize(sequence)\r\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\r\n",
    "input_ids = torch.tensor([ids])\r\n",
    "output = model(input_ids)\r\n",
    "\r\n",
    "print(\"Input_ids\")\r\n",
    "print(input_ids)\r\n",
    "print(\"logits\")\r\n",
    "print(output.logits)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input_ids\n",
      "tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,  1012]])\n",
      "logits\n",
      "tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "여러 문장을 처리하는 것은 batching이라 하며 문장을 하나로 묶어서 인풋으로 넣어줌!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "batch_ids = [ids,ids]\r\n",
    "input_ids = torch.tensor(batch_ids)\r\n",
    "output = model(input_ids)\r\n",
    "\r\n",
    "print(\"Input_ids\")\r\n",
    "print(input_ids)\r\n",
    "print(\"logits\")\r\n",
    "print(output.logits)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input_ids\n",
      "tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,  1012],\n",
      "        [ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,  1012]])\n",
      "logits\n",
      "tensor([[-2.7276,  2.8789],\n",
      "        [-2.7276,  2.8789]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "단 tensor는 직사각형 형태여야 하므로, 길이가 다르면 tensor로 변환이 불가하다"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Padding\r\n",
    "길이가 다른 sequence에 대해 padding을 해주면 해결이 가능하다"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "source": [
    "#two-setences\r\n",
    "batched_ids = [\r\n",
    "  [200, 200, 200],\r\n",
    "  [200, 200],\r\n",
    "]\r\n",
    "\r\n",
    "#padding id\r\n",
    "padding_id = 100\r\n",
    "batched_ids[1].append(padding_id)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "pad token은 tokenizer.pad_token_id로 찾을 수 있음"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\r\n",
    "\r\n",
    "sequence1_ids = [[200, 200, 200]]\r\n",
    "sequence2_ids = [[200, 200]]\r\n",
    "batched_ids = [[200, 200, 200], [200, 200, tokenizer.pad_token_id]]\r\n",
    "\r\n",
    "print(tokenizer.pad_token_id)\r\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\r\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\r\n",
    "print(model(torch.tensor(batched_ids)).logits)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "우리가 생각하기에, seq1 + seq2 == batched_ids여야 하지만 실제로는 seq2가 다른 결과인 것을 볼 수 있다.\r\n",
    "\r\n",
    "이는 padding token이 영향을 준 것이다. 따라서 attention mask를 사용하여 패딩 토큰을 무시하면 된다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Attention masks\r\n",
    "입력 텐서에 대해 0,1로 구분하는데 여기서 1은 연산을 진행하는 값들이고, 0은 무시하는 값이다"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "source": [
    "batched_ids = [\r\n",
    "    [200, 200, 200],\r\n",
    "    [200, 200, tokenizer.pad_token_id]\r\n",
    "]\r\n",
    "\r\n",
    "attention_mask = [\r\n",
    "  [1, 1, 1],\r\n",
    "  [1, 1, 0]\r\n",
    "]\r\n",
    "\r\n",
    "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\r\n",
    "print(outputs.logits)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "source": [
    "#try out\r\n",
    "raw_inputs = [\"I’ve been waiting for a HuggingFace course my whole life.\",\r\n",
    "\"I hate this so much!\"]\r\n",
    "\r\n",
    "#tokenization\r\n",
    "raw1 = tokenizer([raw_inputs[0]], return_tensors=\"pt\")\r\n",
    "raw2 = tokenizer([raw_inputs[1]], return_tensors=\"pt\")\r\n",
    "\r\n",
    "tokenized_inputs = tokenizer(inputs,padding=True, return_tensors=\"pt\")\r\n",
    "tokenized_inputs\r\n",
    "raw1\r\n",
    "print(\"raw1\")\r\n",
    "print(model(**raw1))\r\n",
    "print(\"raw2\")\r\n",
    "print(model(**raw2))\r\n",
    "print(\"all\")\r\n",
    "print(model(**tokenized_inputs))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "raw1\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5979,  1.6390]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "raw2\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 4.1692, -3.3464]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "all\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5979,  1.6390],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Longer sequences\r\n",
    "- BERT 기준 sequence는 512에서 1024개의 token 개수로 제한되어 있음\r\n",
    "- 더 긴 문장을 해결하는 방법\r\n",
    "  - 1) 더 긴 문장을 다룰 수 있는 모델 사용\r\n",
    "    - [Longformer](https://huggingface.co/transformers/model_doc/longformer.html), [LED](https://huggingface.co/transformers/model_doc/led.html)\r\n",
    "  - 2) truncation\r\n",
    "    - sequence = sequence[:max_sequence_length]\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Putting it all together\r\n",
    "- 이전 과정들에서 토크나이저 작동 방법, 토큰화, input_ids로 변환, 패딩, truncation등을 살펴봤는데\r\n",
    "- transformers API에서는 이러한 기능들을 모두 제공"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "source": [
    "from transformers import AutoTokenizer\r\n",
    "\r\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\r\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\r\n",
    "\r\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\r\n",
    "\r\n",
    "model_inputs = tokenizer(sequence)\r\n",
    "model_inputs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "metadata": {},
     "execution_count": 153
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "model input 변수로는 model 작동하기 위해 다양한 attribute가 있는데, distillbert의 경우는 attention mask도 필요한 경우가 있다.\r\n",
    "\r\n",
    "상기 방법에서 multi-sequences에 대햇도 API변경 없이 그대로 사용할 수 있다"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "source": [
    "sequences = [\r\n",
    "  \"I've been waiting for a HuggingFace course my whole life.\",\r\n",
    "  \"So have I!\"\r\n",
    "]\r\n",
    "\r\n",
    "model_inputs = tokenizer(sequences)\r\n",
    "model_inputs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "metadata": {},
     "execution_count": 155
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#padding을 해주는 방법은 다양하다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "source": [
    "# Will pad the sequences up to the maximum sequence length\r\n",
    "model_inputs = tokenizer(sequences, padding=\"longest\")\r\n",
    "# longest : default\r\n",
    "print(model_inputs)\r\n",
    "print(\"*\"*50)\r\n",
    "# Will pad the sequences up to the model max length\r\n",
    "# (512 for BERT or DistilBERT)\r\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\")\r\n",
    "print(model_inputs)\r\n",
    "print(\"*\"*50)\r\n",
    "# Will pad the sequences up to the specified max length\r\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)\r\n",
    "print(model_inputs)\r\n",
    "print(\"*\"*50)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n",
      "**************************************************\n",
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2061, 2031, 1045, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n",
      "**************************************************\n",
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0]]}\n",
      "**************************************************\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "긴 문장에 대해선 다음과 같이 자를 수 있다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "source": [
    "sequences = [\r\n",
    "  \"I've been waiting for a HuggingFace course my whole life.\",\r\n",
    "  \"So have I!\"\r\n",
    "]\r\n",
    "\r\n",
    "# Will truncate the sequences that are longer than the model max length\r\n",
    "# (512 for BERT or DistilBERT)\r\n",
    "model_inputs = tokenizer(sequences, truncation=True)\r\n",
    "print(model_inputs)\r\n",
    "print(\"*\"*50)\r\n",
    "# Will truncate the sequences that are longer than the specified max length\r\n",
    "model_inputs = tokenizer(sequences, max_length=8, truncation=True)\r\n",
    "print(model_inputs)\r\n",
    "print(\"*\"*50)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n",
      "**************************************************\n",
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n",
      "**************************************************\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "tokenzer의 return 형태로는 세 가지가 가능하다\r\n",
    "- numpy arrays : \"np\"\r\n",
    "- PyTorch tensors : \"pt\"\r\n",
    "- TensorFlow tensors : \"tf\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "source": [
    "sequences = [\r\n",
    "  \"I've been waiting for a HuggingFace course my whole life.\",\r\n",
    "  \"So have I!\"\r\n",
    "]\r\n",
    "\r\n",
    "# Returns PyTorch tensors\r\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\")\r\n",
    "print(model_inputs)\r\n",
    "print(\"*\"*50)\r\n",
    "# Returns TensorFlow tensors\r\n",
    "# model_inputs = tokenizer(sequences, padding=True, return_tensors=\"tf\")\r\n",
    "# print(model_inputs)\r\n",
    "# print(\"*\"*50)\r\n",
    "# Returns NumPy arrays\r\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\")\r\n",
    "print(model_inputs)\r\n",
    "print(\"*\"*50)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "**************************************************\n",
      "{'input_ids': array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,\n",
      "        12172,  2607,  2026,  2878,  2166,  1012,   102],\n",
      "       [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "**************************************************\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Special tokens\r\n",
    "- 토크나이저 출력 결과를 보면 기존과 다른 것을 알 수 있음\r\n",
    "- 시작 부분에 [CLS], 끝에는 [SEP] 스페셜 토큰이 추가됨\r\n",
    "- 이는 모델의 전처리 방법을 따르기 때문\r\n",
    "- 모델에 따라 스페셜 토큰이 다름"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "source": [
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\r\n",
    "\r\n",
    "model_inputs = tokenizer(sequence)\r\n",
    "print(model_inputs[\"input_ids\"])\r\n",
    "\r\n",
    "tokens = tokenizer.tokenize(sequence)\r\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\r\n",
    "print(ids,\"\\n\")\r\n",
    "\r\n",
    "print(tokenizer.decode(model_inputs[\"input_ids\"]))\r\n",
    "print(tokenizer.decode(ids))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]\n",
      "[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012] \n",
      "\n",
      "[CLS] i've been waiting for a huggingface course my whole life. [SEP]\n",
      "i've been waiting for a huggingface course my whole life.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wrapping up: From tokenizer to model\r\n",
    "이제 토크나이저로 전처리 시 모든 간계를 알았기 한 번 해보자고오오오"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "source": [
    "import torch\r\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\r\n",
    "\r\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\r\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\r\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\r\n",
    "\r\n",
    "sequences = [\r\n",
    "  \"I've been waiting for a HuggingFace course my whole life.\",\r\n",
    "  \"So have I!\"\r\n",
    "]\r\n",
    "\r\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\r\n",
    "output = model(**tokens)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "source": [
    "import numpy as np\r\n",
    "output_ = torch.nn.functional.softmax(output.logits,dim=-1)\r\n",
    "np.round(output_.detach().numpy())\r\n",
    "# output"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 207
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "9d4b788d31f1e8a98acbf5edfe42af7a280714d85bd73a1cdc11a1af0199f3f5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}